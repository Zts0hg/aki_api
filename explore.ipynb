{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import hashlib\n",
    "import time\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import sys\n",
    "import text_to_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kanji_with_kana_pattern = re.compile(r\"\\s*(?P<kanji>[\\u4E00-\\u9FFF]) (?P<kana>[\\u3040-\\u309F]+)\\s*\")\n",
    "\n",
    "\n",
    "def transform_content_to_ruby_rich_text(content):\n",
    "    return kanji_with_kana_pattern.sub(r\"<ruby>\\1<rt>\\2</rt></ruby>\", content)\n",
    "\n",
    "\n",
    "def split_kanji_and_its_kana(content):\n",
    "    return kanji_with_kana_pattern.sub(r\"\\1\", content), kanji_with_kana_pattern.sub(r\"\\2\", content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_md5(content):\n",
    "    content = content.encode(\"utf-8\")\n",
    "    md5_hash = hashlib.md5()\n",
    "    md5_hash.update(content)\n",
    "    return md5_hash.hexdigest()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "sentence_content_pattern = re.compile(r'([\\u2460-\\u2470])\\s*')\n",
    "invalid_line_pattern = re.compile(r'^[\\d\\s]+$')\n",
    "\n",
    "example_sentences = []\n",
    "groups = []\n",
    "with open(\"CS_Word_20240405_22.25.17.txt\", \"r\", encoding=\"utf-8\") as fp:\n",
    "    matched = False\n",
    "    sentence = None\n",
    "    last_line = \"\"\n",
    "    for line in fp.readlines():\n",
    "        line = line.strip()\n",
    "        if len(line) < 3 or invalid_line_pattern.match(line.strip()):\n",
    "            continue\n",
    "\n",
    "        if matched == True and last_line[-1] != \"。\":\n",
    "            sentence[\"content\"] += line\n",
    "            last_line = line\n",
    "            continue\n",
    "\n",
    "        if sentence_content_pattern.match(line.strip()):\n",
    "            matched = True\n",
    "            sentence = {\"content\": sentence_content_pattern.sub(r\"\\1 \", line)}\n",
    "        else:\n",
    "            if matched:\n",
    "                sentence[\"hiragana\"] = \"\"\n",
    "                sentence[\"meaning\"] = line\n",
    "                if sentence[\"meaning\"][1] == \" \":\n",
    "                    sentence[\"meaning\"] = sentence[\"meaning\"][2:]\n",
    "\n",
    "                if sentence[\"meaning\"][0] == \"囉\":\n",
    "                    sentence[\"meaning\"] = sentence[\"meaning\"][1:]\n",
    "\n",
    "                if sentence[\"content\"][0] == \"①\" and groups:\n",
    "                    example_sentences.append(groups)\n",
    "                    groups = []\n",
    "\n",
    "                groups.append(sentence)\n",
    "            matched = False\n",
    "        last_line = line\n",
    "\n",
    "if groups:\n",
    "    example_sentences.append(groups)\n",
    "\n",
    "print(example_sentences[1])\n",
    "with open(\"example_sentences.json\", \"w\", encoding=\"utf-8\") as fp:\n",
    "    fp.write(json.dumps(example_sentences, ensure_ascii=False, indent=4))\n",
    "    # pd.DataFrame(example_sentences, dtype=str).fillna(\"\").to_json(fp, orient=\"records\", force_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json(\"grammar_pd.json\", dtype=str)\n",
    "with open(\"grammar.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.loads(f.read())\n",
    "    df = pd.DataFrame(data)\n",
    "df.head()\n",
    "detail = df.loc[0].to_dict()\n",
    "print(type(detail[\"example\"]))\n",
    "print(detail[\"example\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from grammar_enumeration import grammars, df_grammars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grammars.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grammars.hiragana.str.replace(r\"[（）()\\s()]\", \"\").str.contains(r'なしに')[23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword = \"なくして(は)\"\n",
    "df = df_grammars\n",
    "df[df.content.str.replace(r\"[（）()\\s()]\", \"\").str.contains(keyword)\n",
    "                    | df.hiragana.str.replace(r\"[（）()\\s()]\", \"\").str.contains(keyword)\n",
    "                    | df.chinese_meaning.str.contains(keyword) | (df.source == keyword)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"其中N1级别201个，N2级别150个，N3级别140个，N4级别130个，N5级别120个。\"\n",
    "\n",
    "import json\n",
    "import re\n",
    "\n",
    "level_pattern = re.compile(r\"^N([1-5])文法\")\n",
    "unit_pattern = re.compile(r\"^第(\\d+)单元(.*)\")\n",
    "grammar_start_pattern = re.compile(r\"^\\d+\\. \")\n",
    "seq_pattern = re.compile(r\"^（\\d+）\")\n",
    "from collections import defaultdict\n",
    "\n",
    "lines = []\n",
    "part_keywords = {'接続': 391, '説明': 401, '例文': 910, '注意': 774, '接续': 308, '说明': 515, '读法': 4, '例词': 2, '补充': 1}\n",
    "level = 0\n",
    "current_grammar = \"\"\n",
    "current_part = \"\"\n",
    "current_seq = \"\"\n",
    "output = False\n",
    "# with open(\"日语蓝宝书.txt\", \"r\", encoding=\"utf-8\") as fp:\n",
    "# for idx, line in enumerate(fp.readlines()):\n",
    "content = \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "89. ～とて\n",
    "\n",
    "\n",
    "（1）\n",
    "\n",
    "接続\n",
    "\n",
    "名＋とて\n",
    "\n",
    "説明\n",
    "\n",
    "表示假定条件的逆接，“即使……也不例外”“即使是……也……”“甚至……”“就连……”。\n",
    "\n",
    "例文\n",
    "\n",
    "△常に冷静な彼とて やはり人間だから、感情的になってしまうこともあるのだろう。 【2009年12月真题】/尽管他平时很冷静，但是也有情绪化的时候吧 。\n",
    "\n",
    "△最近の電気製品は機能が多すぎる。開発者たちとて すべての機能が必要とは思わないのではないか。 【2007年真题】/最近的电器功能实在太多，就算是开发商也未必会认为所有的功能都有必要吧 。\n",
    "\n",
    "△私とて 試合に負けたことに悔しい。 /输掉了比赛，我也很懊恼 。\n",
    "\n",
    "注意\n",
    "\n",
    "①前面主要接续人名。\n",
    "\n",
    "②意思与「～としても」 相同，但「～とて」 是比较生硬的表达方式。\n",
    "\n",
    "（2）\n",
    "\n",
    "接続\n",
    "\n",
    "名＋だ/動た形＋とて\n",
    "\n",
    "説明\n",
    "\n",
    "表示假定条件的逆接，无论前项怎样，后项的原则都是一样的。“即使……也……”“即便……也……”“就算……也……”。\n",
    "\n",
    "例文\n",
    "\n",
    "△たとえ病気だとて 試験に欠席してはいけない。 /就算生病了也不能缺考 。\n",
    "\n",
    "△いくら 頼 たの んだとて 、できないことはできない。 /再怎么拜托，不能做的事情就是不能做 。\n",
    "\n",
    "△どんなに後悔したとて 、過ぎたことは今さらどうしようもない。 /就算再怎么后悔，对于已经过去的事情也没有办法 。\n",
    "\n",
    "注意\n",
    "\n",
    "常与「たとえ」 「どんなに」 「いくら」 等词一起使用。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "grammars = {}\n",
    "for _ in range(1):\n",
    "    for idx, line in enumerate(content.split(\"\\n\")):\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        if res := level_pattern.findall(line):\n",
    "            level = int(res[0])\n",
    "            # print(f\"Current Level: {level}\")\n",
    "            continue\n",
    "\n",
    "        if res := unit_pattern.findall(line):\n",
    "            # print(f\"Current Unit: {res[0][0]}.{res[0][1]}\")\n",
    "            continue\n",
    "\n",
    "        if grammar_start_pattern.match(line):\n",
    "            print(f\"Current Grammar: {line}\")\n",
    "            current_grammar = line\n",
    "            grammars[current_grammar] = {}\n",
    "            # current_part = \"\"\n",
    "            # current_seq = \"\"\n",
    "            continue\n",
    "\n",
    "        if seq_pattern.match(line):\n",
    "            # print(f\"Current Seq: {line}\")\n",
    "            current_seq = line\n",
    "            continue\n",
    "\n",
    "        lines.append(line)\n",
    "\n",
    "        if len(line) < 5 and line[:2] in part_keywords:\n",
    "            current_part = line[:2]\n",
    "            if current_part == \"例文\":\n",
    "                grammars[current_grammar][current_part] = []\n",
    "            else:\n",
    "                grammars[current_grammar][current_part] = \"\"\n",
    "            continue\n",
    "\n",
    "        if current_part == \"例文\":\n",
    "            grammars[current_grammar][current_part].append(line)\n",
    "        else:\n",
    "            grammars[current_grammar][current_part] += line + \"\\n\"\n",
    "\n",
    "print(json.dumps(grammars, ensure_ascii=False, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"其中N1级别201个，N2级别150个，N3级别140个，N4级别130个，N5级别120个。\"\n",
    "\n",
    "import json\n",
    "import re\n",
    "\n",
    "content = \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "89. ～とて\n",
    "\n",
    "\n",
    "（1）\n",
    "\n",
    "接続\n",
    "\n",
    "名＋とて\n",
    "\n",
    "説明\n",
    "\n",
    "表示假定条件的逆接，“即使……也不例外”“即使是……也……”“甚至……”“就连……”。\n",
    "\n",
    "例文\n",
    "\n",
    "△常に冷静な彼とて やはり人間だから、感情的になってしまうこともあるのだろう。 【2009年12月真题】/尽管他平时很冷静，但是也有情绪化的时候吧 。\n",
    "\n",
    "△最近の電気製品は機能が多すぎる。開発者たちとて すべての機能が必要とは思わないのではないか。 【2007年真题】/最近的电器功能实在太多，就算是开发商也未必会认为所有的功能都有必要吧 。\n",
    "\n",
    "△私とて 試合に負けたことに悔しい。 /输掉了比赛，我也很懊恼 。\n",
    "\n",
    "注意\n",
    "\n",
    "①前面主要接续人名。\n",
    "\n",
    "②意思与「～としても」 相同，但「～とて」 是比较生硬的表达方式。\n",
    "\n",
    "（2）\n",
    "\n",
    "接続\n",
    "\n",
    "名＋だ/動た形＋とて\n",
    "\n",
    "説明\n",
    "\n",
    "表示假定条件的逆接，无论前项怎样，后项的原则都是一样的。“即使……也……”“即便……也……”“就算……也……”。\n",
    "\n",
    "例文\n",
    "\n",
    "△たとえ病気だとて 試験に欠席してはいけない。 /就算生病了也不能缺考 。\n",
    "\n",
    "△いくら 頼 たの んだとて 、できないことはできない。 /再怎么拜托，不能做的事情就是不能做 。\n",
    "\n",
    "△どんなに後悔したとて 、過ぎたことは今さらどうしようもない。 /就算再怎么后悔，对于已经过去的事情也没有办法 。\n",
    "\n",
    "注意\n",
    "\n",
    "常与「たとえ」 「どんなに」 「いくら」 等词一起使用。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "level_pattern = re.compile(r\"^N([1-5])文法\")\n",
    "unit_pattern = re.compile(r\"^第(\\d+)单元(.*)\")\n",
    "grammar_start_pattern = re.compile(r\"^\\d+\\. \")\n",
    "seq_pattern = re.compile(r\"^（\\d+）\")\n",
    "\n",
    "\n",
    "def get_japanese_sequence_sign(number):\n",
    "    number = int(number)\n",
    "    if number <= 0:\n",
    "        return \"⓪\"\n",
    "\n",
    "    return chr(ord(\"\\u2460\") + number - 1)\n",
    "\n",
    "\n",
    "def extract_grammars(content):\n",
    "    grammars = {}\n",
    "    lines = []\n",
    "    part_keywords = {'接続': 391, '説明': 401, '例文': 910, '注意': 774, '接续': 308, '说明': 515, '读法': 4, '例词': 2, '补充': 1}\n",
    "    keyword_mapping = {\n",
    "        '接続': \"接续\",\n",
    "        '説明': \"说明\",\n",
    "        '读法': \"说明\",\n",
    "        '例词': \"说明\",\n",
    "        '补充': \"注意\",\n",
    "    }\n",
    "    level = 0\n",
    "    current_grammar = \"\"\n",
    "    current_part = \"\"\n",
    "    current_seq = \"\"\n",
    "    for idx, line in enumerate(content.split(\"\\n\")):\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        line = line.replace(\"真题】\", \"JLPT】\")\n",
    "        if res := level_pattern.findall(line):\n",
    "            level = int(res[0])\n",
    "            # print(f\"Current Level: {level}\")\n",
    "            continue\n",
    "\n",
    "        if res := unit_pattern.findall(line):\n",
    "            # print(f\"Current Unit: {res[0][0]}.{res[0][1]}\")\n",
    "            continue\n",
    "\n",
    "        if grammar_start_pattern.match(line):\n",
    "            # print(f\"Current Grammar: {line}\")\n",
    "            current_grammar = line\n",
    "            grammars[current_grammar] = {}\n",
    "            grammars[current_grammar][\"level\"] = level\n",
    "            # current_part = \"\"\n",
    "            # current_seq = \"\"\n",
    "            continue\n",
    "\n",
    "        if seq_pattern.match(line):\n",
    "            print(f\"Current Seq: {line}\")\n",
    "            current_seq = line\n",
    "            continue\n",
    "\n",
    "        lines.append(line)\n",
    "\n",
    "        if len(line) < 5 and line[:2] in part_keywords:\n",
    "            current_part = line[:2]\n",
    "            current_part = keyword_mapping.get(current_part, current_part)\n",
    "            index = f\"{get_japanese_sequence_sign(line[2])} \" if len(line) > 2 else \"\"\n",
    "            if current_part not in grammars[current_grammar]:\n",
    "                if current_part == \"例文\":\n",
    "                    grammars[current_grammar][current_part] = []\n",
    "                else:\n",
    "                    grammars[current_grammar][current_part] = \"\"\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            if current_part == \"例文\":\n",
    "                grammars[current_grammar][current_part].append(line)\n",
    "            else:\n",
    "                grammars[current_grammar][current_part] += index + line + \"\\n\"\n",
    "                index = \"\"\n",
    "        except Exception:\n",
    "            print((idx, line))\n",
    "            raise\n",
    "\n",
    "    return grammars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dumps(\"⓪①②③\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chr(ord(\"\\u2460\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kanji_with_kana_pattern = re.compile(r\"(?P<kanji>[\\u4E00-\\u9FFF]) (?P<kana>[\\u3040-\\u309F]+) \")\n",
    "content = \"中 なか 川 がわ 先 せん 生 せい 「あ、 山 やま 口 ぐち さん。 偶 ぐう 然 ぜん ですね。」\"\n",
    "content = \"辞书形 ⇒ 尊他语(辞书形)\"\n",
    "content = \"做 | する ⇒ なさる\"\n",
    "content = \"84. ～ 中 ちゅう/じゅう\"\n",
    "print(kanji_with_kana_pattern.findall(content))\n",
    "kanji_with_kana_pattern.sub(r\"<ruby>\\1<rt>\\2</rt></ruby>\", content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "content = \"△お 客 きゃく 様 さま 、このお 皿 さら をさげてもよろしいでしょうか 。 /客人您好，这个盘子我可以撤下去了吗 ？\"\n",
    "print(transform_content_to_ruby_rich_text(content))\n",
    "\n",
    "\n",
    "print(split_kanji_and_its_kana(content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_example_sentences(sentences):\n",
    "    new_sentences = []\n",
    "    for sentence in sentences:\n",
    "        if not sentence.startswith(\"△\"):\n",
    "            new_sentences[-1] += \"\\n\" + sentence\n",
    "        elif sentence.startswith(\"△B\"):\n",
    "            new_sentences[-1] += \"\\n  \" + sentence[1:]\n",
    "        else:\n",
    "            new_sentences.append(sentence)\n",
    "\n",
    "    return new_sentences\n",
    "\n",
    "\n",
    "with open(\"日语蓝宝书.txt\", \"r\", encoding=\"utf-8\") as fp:\n",
    "    content = fp.read()\n",
    "\n",
    "grammars = extract_grammars(content)\n",
    "# print(json.dumps(grammars, ensure_ascii=False, indent=4))\n",
    "part_keywords = {'接続': 391, '説明': 401, '例文': 910, '注意': 774, '接续': 308, '说明': 515, '读法': 4, '例词': 2, '补充': 1}\n",
    "example_sentence_pattern = re.compile(r\"^([^/]+?)(【\\d+年(?:\\d+月)?JLPT】)?/(.*)$\", re.DOTALL)\n",
    "header_pattern = re.compile(r\"^△\\s*\")\n",
    "formatted_grammars = []\n",
    "for idx, (grammar, detail) in enumerate(grammars.items()):\n",
    "    try:\n",
    "        content = grammar_start_pattern.sub(\"\", grammar)\n",
    "        hiragana = \"\"\n",
    "        rich_text = transform_content_to_ruby_rich_text(content)\n",
    "        if rich_text != content:\n",
    "            content, hiragana = split_kanji_and_its_kana(content)\n",
    "\n",
    "        example_sentences = detail.get(\"例文\") or []\n",
    "        example_sentences = merge_example_sentences(example_sentences)\n",
    "        formated_exmpale_sentences = []\n",
    "\n",
    "        for sentence_seq, sentence in enumerate(example_sentences):\n",
    "            sentence_content, tag, meaning = example_sentence_pattern.findall(sentence)[0]\n",
    "            sentence_content = header_pattern.sub(\"\", sentence_content)\n",
    "            sentence_content = f\"{get_japanese_sequence_sign(sentence_seq+1)} {sentence_content}\"\n",
    "\n",
    "            sentence_content = transform_content_to_ruby_rich_text(sentence_content)\n",
    "            formated_exmpale_sentences.append({\"content\": sentence_content, \"tag\": tag, \"meaning\": meaning})\n",
    "\n",
    "        # example_sentences = [{\"content\": sentence} for sentence in map(transform_content_to_ruby_rich_text, detail.get(\"例文\") or [])]\n",
    "\n",
    "        formatted_grammars.append({\n",
    "            \"id\": idx + 1,\n",
    "            \"content\": content,\n",
    "            \"hiragana\": hiragana,\n",
    "            \"meaning\": detail.get(\"说明\", \"\").strip(),\n",
    "            \"usage\": detail.get(\"接续\", \"\").strip(),\n",
    "            \"example\": formated_exmpale_sentences,\n",
    "            \"remark\": detail.get(\"注意\", \"\").strip(),\n",
    "            \"source\": \"S2N1-N5\",\n",
    "            \"japanese_meaning\": \"\",\n",
    "            \"chinese_meaning\": detail.get(\"说明\", \"\").strip(),\n",
    "            \"level\": detail[\"level\"]\n",
    "        })\n",
    "    except Exception:\n",
    "        print(grammar)\n",
    "        print(content)\n",
    "        print(detail)\n",
    "        raise\n",
    "\n",
    "print(formatted_grammars[0])\n",
    "with open(\"grammars_N1-N5.json\", \"w\", encoding=\"utf-8\") as fp:\n",
    "    fp.write(json.dumps(formatted_grammars, ensure_ascii=False, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 日语平假名unicode编码范围：\\u3040-\\u309F\n",
    "# 日语片假名unicode编码范围：\\u30A0-\\u30FF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = \"私の料理を一口食べるなり 、父は変な顔をして席を立ってしまった。\"\n",
    "get_md5(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import sys\n",
    "\n",
    "pattern = re.compile(r\"^[△\\u2460-\\u2470\\s]*\")\n",
    "rich_text_pattern = re.compile(r\"<ruby>(.*?)<rt>.*?</rt></ruby>\")\n",
    "RE_WHITESPACES_PATTERN = re.compile(r\"\\s+\")\n",
    "\n",
    "with open(\"grammars_N1-N5.json\", \"r\", encoding=\"utf-8\") as fp:\n",
    "    formatted_grammars = json.loads(fp.read())\n",
    "\n",
    "total_sentence_amount = sum([len(grammar[\"example\"]) for grammar in formatted_grammars])\n",
    "print(f\"Total sentence amount: {total_sentence_amount}\")\n",
    "\n",
    "for grammar in tqdm(formatted_grammars, file=sys.stdout):\n",
    "    for sentence in grammar[\"example\"]:\n",
    "        content = sentence[\"content\"]\n",
    "        content = pattern.sub(\"\", content)\n",
    "        content = rich_text_pattern.sub(r\"\\1\", content)\n",
    "        content = RE_WHITESPACES_PATTERN.sub(\"\", content)\n",
    "\n",
    "        content_md5 = get_md5(content)\n",
    "        audio_path = f\"japanese_grammar/audio/{content_md5}.mp3\"\n",
    "\n",
    "        text_to_audio.generate_audio([\n",
    "            (None, content),\n",
    "        ], audio_path, log=tqdm.write)\n",
    "        time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar_content_pattern = re.compile(r\"^\\d? ?[~\\u3040-\\u309F\\u30A0-\\u30FF\\u4E00-\\u9FFF  ()]+\")\n",
    "\n",
    "grammar_content_pattern.findall(\"~ 際 (に)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = \"ここに車を止められるのは、許可をもらっている人（a だけ　b に限り）です。\"\n",
    "content = re.sub(\"\\s+\", \" \", content)\n",
    "print(content)\n",
    "\n",
    "question_pattern = re.compile(r\"[（(](?:[\\da-n]\\s*[~\\u3040-\\u309F\\u30A0-\\u30FF\\u4E00-\\u9FFF]+\\s*)+[)）]\")\n",
    "print(question_pattern.findall(content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(r\"(?:[\\da-n]\\s*[~\\u3040-\\u309F\\u30A0-\\u30FF\\u4E00-\\u9FFF]+\\s*)+\", \"a だけ b に限り\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_pattern = re.compile(r\"[(（](?:[\\da-n]\\s*[~\\u3040-\\u309F\\u30A0-\\u30FF\\u4E00-\\u9FFF]+\\s*)+[）)]*\\?\")\n",
    "print(question_pattern.findall(\"こちらの会議室をご利用になる際は、 受付で必要事項をご記入ください。\"))\n",
    "print(question_pattern.findall(\"ここに車を止められるのは、許可をもらっている人（a だけ　b に限り）です。\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "grammar_content_pattern = re.compile(r\"^\\d [~\\u3040-\\u309F\\u30A0-\\u30FF\\u4E00-\\u9FFF  ()]+\")\n",
    "sentence_content_pattern = re.compile(r'([\\u2460-\\u2470])\\s*')\n",
    "invalid_line_pattern = re.compile(r'^[\\d\\s]+$')\n",
    "\n",
    "example_sentences = []\n",
    "with open(\"CS_Word_20240403_23.41.17.txt\", \"r\", encoding=\"utf-8\") as fp:\n",
    "    matched = False\n",
    "    sentence = None\n",
    "    last_line = \"\"\n",
    "    for line in fp.readlines():\n",
    "        line = line.strip()\n",
    "        if len(line) < 3 or invalid_line_pattern.match(line.strip()):\n",
    "            continue\n",
    "\n",
    "        if grammar_content_pattern.match(line):\n",
    "            print(line)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
