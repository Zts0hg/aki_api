{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import hashlib\n",
    "import time\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import sys\n",
    "from config import japanese_grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import text_to_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kanji_with_kana_pattern = re.compile(\n",
    "    r\"\\s*(?P<kanji>[\\u4E00-\\u9FFF]) (?P<kana>[\\u3040-\\u309F]+)\\s*\"\n",
    ")\n",
    "\n",
    "\n",
    "def transform_content_to_ruby_rich_text(content, pattern=None):\n",
    "    if pattern is None:\n",
    "        pattern = kanji_with_kana_pattern\n",
    "\n",
    "    return pattern.sub(r\"<ruby>\\1<rt>\\2</rt></ruby>\", content)\n",
    "\n",
    "\n",
    "def split_kanji_and_its_kana(content, pattern=None):\n",
    "    if pattern is None:\n",
    "        pattern = kanji_with_kana_pattern\n",
    "\n",
    "    return kanji_with_kana_pattern.sub(r\"\\1\", content), kanji_with_kana_pattern.sub(\n",
    "        r\"\\2\", content\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kanji_with_kana_pattern = re.compile(\n",
    "    r\"(?P<kanji>[\\u4E00-\\u9FFF])(?P<kana>[\\u3040-\\u309F]+)(?:\\s|$)\"\n",
    ")\n",
    "content = \"\"\"\n",
    "▶ あかい「赤い」 ②⓪（形）：红色的\n",
    "\n",
    "解 古代日本人不区分“红”和“亮”两个概念，因为明亮的太阳光和火光都偏红色。\n",
    "\n",
    "例 赤あか い花はな が咲さ いている。/红色的花开了。\n",
    "\"\"\"\n",
    "\n",
    "print(transform_content_to_ruby_rich_text(content, pattern=kanji_with_kana_pattern))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kana_meaning_pattern = re.compile(r\"^\\d{2,}\\s\")\n",
    "example_pattern = re.compile(r\"^▶\")\n",
    "\n",
    "result = []\n",
    "kanji_with_kana_pattern = re.compile(\n",
    "    r\"(?P<kanji>[\\u4E00-\\u9FFF]+)(?P<kana>[\\u3040-\\u309F]+)(?:\\s|$)\"\n",
    ")\n",
    "file_path = (\n",
    "    japanese_grammar.book_folder\n",
    "    / \"记单词，一定要学的130个日语词根-_张铭_-_Z-Library_.txt\"\n",
    ")\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as fp:\n",
    "    current_kana = \"\"\n",
    "    current_meaning_item = \"\"\n",
    "    current_meanning_explain = []\n",
    "    current_example = \"\"\n",
    "    current_example_explain = []\n",
    "    within_example_part = False\n",
    "\n",
    "    lines = fp.readlines()\n",
    "    last_line = \"\"\n",
    "    for line in lines:\n",
    "        content = line.strip()\n",
    "        if not content:\n",
    "            continue\n",
    "\n",
    "        if current_kana and content.startswith(\"第二章\"):\n",
    "            break\n",
    "\n",
    "        # 假名\n",
    "        if len(content) == 1:\n",
    "            current_kana = content\n",
    "            current_meaning_item = \"\"\n",
    "            current_meanning_explain = []\n",
    "            current_example = \"\"\n",
    "            current_example_explain = []\n",
    "            within_example_part = False\n",
    "            continue\n",
    "\n",
    "        # 假名词根含义\n",
    "        if current_kana and kana_meaning_pattern.match(content):\n",
    "            if current_example:\n",
    "                result.append(\n",
    "                    {\n",
    "                        \"kana\": current_kana,\n",
    "                        \"meaning\": current_meaning_item,\n",
    "                        \"explain\": \"<br>\".join(current_meanning_explain),\n",
    "                        \"example\": current_example,\n",
    "                        \"example_explain\": \"<br>\".join(\n",
    "                            [\n",
    "                                transform_content_to_ruby_rich_text(\n",
    "                                    content, pattern=kanji_with_kana_pattern\n",
    "                                )\n",
    "                                for content in current_example_explain\n",
    "                            ]\n",
    "                        ),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            current_meaning_item = content\n",
    "            current_meanning_explain = []\n",
    "            current_example = \"\"\n",
    "            within_example_part = False\n",
    "            item_idx = content.split()[0]\n",
    "            continue\n",
    "\n",
    "        # 词根含义举例\n",
    "        if current_kana and example_pattern.match(content):\n",
    "            within_example_part = True\n",
    "            if current_example:\n",
    "                result.append(\n",
    "                    {\n",
    "                        \"kana\": current_kana,\n",
    "                        \"meaning\": current_meaning_item,\n",
    "                        \"explain\": \"<br>\".join(current_meanning_explain),\n",
    "                        \"example\": current_example,\n",
    "                        \"example_explain\": \"<br>\".join(\n",
    "                            [\n",
    "                                transform_content_to_ruby_rich_text(\n",
    "                                    content, pattern=kanji_with_kana_pattern\n",
    "                                )\n",
    "                                for content in current_example_explain\n",
    "                            ]\n",
    "                        ),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            current_example = content\n",
    "            current_example_explain = []\n",
    "            continue\n",
    "\n",
    "        # 词根举例 解释\n",
    "        if current_example:\n",
    "            current_example_explain.append(content)\n",
    "            continue\n",
    "\n",
    "        # 假名词根 解释\n",
    "        if current_kana and not within_example_part:\n",
    "            current_meanning_explain.append(content)\n",
    "\n",
    "        last_line = content\n",
    "\n",
    "\n",
    "if current_example_explain:\n",
    "    result.append(\n",
    "        {\n",
    "            \"kana\": current_kana,\n",
    "            \"meaning\": current_meaning_item,\n",
    "            \"explain\": \"<br>\".join(current_meanning_explain),\n",
    "            \"example\": current_example,\n",
    "            \"example_explain\": \"<br>\".join(\n",
    "                [\n",
    "                    transform_content_to_ruby_rich_text(\n",
    "                        content, pattern=kanji_with_kana_pattern\n",
    "                    )\n",
    "                    for content in current_example_explain\n",
    "                ]\n",
    "            ),\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for record in result:\n",
    "    if record[\"example\"] == \"▶ あか「垢」 ②（名）：污垢；水锈\":\n",
    "        print(record)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(result).fillna(\"\")\n",
    "print(df.shape)\n",
    "df.to_csv(\"word_roots.csv\", index=False)\n",
    "df.to_csv(\"word_roots_anki.csv\", index=False, header=False)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_kana = \"\"\n",
    "last_meaning = \"\"\n",
    "for index in df.index:\n",
    "    detail = df.loc[index].to_dict()\n",
    "    if detail[\"kana\"] != last_kana:\n",
    "        print(\"=\" * 32)\n",
    "        print(detail[\"kana\"])\n",
    "    if detail[\"meaning\"] != last_meaning:\n",
    "        print()\n",
    "        print(detail[\"meaning\"])\n",
    "        print(detail[\"explain\"])\n",
    "\n",
    "    example_explain = detail[\"example_explain\"].split(\"<br>\")[0]\n",
    "    example_explain = \"\" if example_explain[0] != \"解\" else \" \".join(example_explain.split()[1:])\n",
    "    print(detail[\"example\"])\n",
    "    if example_explain:\n",
    "        print(example_explain)\n",
    "\n",
    "    last_kana = detail[\"kana\"]\n",
    "    last_meaning = detail[\"meaning\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(719, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>word_explain</th>\n",
       "      <th>kana_meaning</th>\n",
       "      <th>kana_meaning_explain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{{c1::あかい}}&lt;br&gt;赤い ②⓪ （形）&lt;br&gt;{{c2::红色的}}</td>\n",
       "      <td>解 古代日本人不区分“红”和“亮”两个概念，因为明亮的太阳光和火光都偏红色。&lt;br&gt;例 &lt;r...</td>\n",
       "      <td>01 「あ」表 示“明亮”，以“あ＋か行假名”为代表</td>\n",
       "      <td>解析 あ是50 音图中发音最洪亮的假名，故有明亮之感。&lt;br&gt;熟词联想 あかい「赤い」：红色的</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{{c1::あか}}&lt;br&gt;赤 ① （名）&lt;br&gt;{{c2::红色}}</td>\n",
       "      <td>例 &lt;ruby&gt;好&lt;rt&gt;す&lt;/rt&gt;&lt;/ruby&gt;きな&lt;ruby&gt;色&lt;rt&gt;いろ&lt;/rt&gt;...</td>\n",
       "      <td>01 「あ」表 示“明亮”，以“あ＋か行假名”为代表</td>\n",
       "      <td>解析 あ是50 音图中发音最洪亮的假名，故有明亮之感。&lt;br&gt;熟词联想 あかい「赤い」：红色的</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{{c1::あかるい}}&lt;br&gt;明るい ⓪③ （形）&lt;br&gt;{{c2::明亮的；明朗的}}</td>\n",
       "      <td>解 あか→明亮。&lt;br&gt;例 &lt;ruby&gt;明&lt;rt&gt;あか&lt;/rt&gt;&lt;/ruby&gt;るい&lt;ruby...</td>\n",
       "      <td>01 「あ」表 示“明亮”，以“あ＋か行假名”为代表</td>\n",
       "      <td>解析 あ是50 音图中发音最洪亮的假名，故有明亮之感。&lt;br&gt;熟词联想 あかい「赤い」：红色的</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{{c1::あきらか}}&lt;br&gt;明らか ② （形动）&lt;br&gt;{{c2::明朗的；清楚的}}</td>\n",
       "      <td>解 あき→明亮，らか→形容动词后缀。&lt;br&gt;例 &lt;ruby&gt;犯人&lt;rt&gt;はんにん&lt;/rt&gt;&lt;...</td>\n",
       "      <td>01 「あ」表 示“明亮”，以“あ＋か行假名”为代表</td>\n",
       "      <td>解析 あ是50 音图中发音最洪亮的假名，故有明亮之感。&lt;br&gt;熟词联想 あかい「赤い」：红色的</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{{c1::あける}}&lt;br&gt;明ける ⓪ （自一）&lt;br&gt;{{c2::天亮了；过年}}</td>\n",
       "      <td>解 あけ→明亮。&lt;br&gt;例 あけまして、おめでとうございます。/过年好。</td>\n",
       "      <td>01 「あ」表 示“明亮”，以“あ＋か行假名”为代表</td>\n",
       "      <td>解析 あ是50 音图中发音最洪亮的假名，故有明亮之感。&lt;br&gt;熟词联想 あかい「赤い」：红色的</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            word  \\\n",
       "0        {{c1::あかい}}<br>赤い ②⓪ （形）<br>{{c2::红色的}}   \n",
       "1            {{c1::あか}}<br>赤 ① （名）<br>{{c2::红色}}   \n",
       "2  {{c1::あかるい}}<br>明るい ⓪③ （形）<br>{{c2::明亮的；明朗的}}   \n",
       "3  {{c1::あきらか}}<br>明らか ② （形动）<br>{{c2::明朗的；清楚的}}   \n",
       "4    {{c1::あける}}<br>明ける ⓪ （自一）<br>{{c2::天亮了；过年}}   \n",
       "\n",
       "                                        word_explain  \\\n",
       "0  解 古代日本人不区分“红”和“亮”两个概念，因为明亮的太阳光和火光都偏红色。<br>例 <r...   \n",
       "1  例 <ruby>好<rt>す</rt></ruby>きな<ruby>色<rt>いろ</rt>...   \n",
       "2  解 あか→明亮。<br>例 <ruby>明<rt>あか</rt></ruby>るい<ruby...   \n",
       "3  解 あき→明亮，らか→形容动词后缀。<br>例 <ruby>犯人<rt>はんにん</rt><...   \n",
       "4               解 あけ→明亮。<br>例 あけまして、おめでとうございます。/过年好。   \n",
       "\n",
       "                 kana_meaning                             kana_meaning_explain  \n",
       "0  01 「あ」表 示“明亮”，以“あ＋か行假名”为代表  解析 あ是50 音图中发音最洪亮的假名，故有明亮之感。<br>熟词联想 あかい「赤い」：红色的  \n",
       "1  01 「あ」表 示“明亮”，以“あ＋か行假名”为代表  解析 あ是50 音图中发音最洪亮的假名，故有明亮之感。<br>熟词联想 あかい「赤い」：红色的  \n",
       "2  01 「あ」表 示“明亮”，以“あ＋か行假名”为代表  解析 あ是50 音图中发音最洪亮的假名，故有明亮之感。<br>熟词联想 あかい「赤い」：红色的  \n",
       "3  01 「あ」表 示“明亮”，以“あ＋か行假名”为代表  解析 あ是50 音图中发音最洪亮的假名，故有明亮之感。<br>熟词联想 あかい「赤い」：红色的  \n",
       "4  01 「あ」表 示“明亮”，以“あ＋か行假名”为代表  解析 あ是50 音图中发音最洪亮的假名，故有明亮之感。<br>熟词联想 あかい「赤い」：红色的  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_type_answer = []\n",
    "\n",
    "kana_pattern = re.compile(r\"(?<=^▶ )([\\u3040-\\u309F]+)(?=「)\")\n",
    "meaning_pattern = re.compile(r\"(?<=：)(.+)$\")\n",
    "content_pattern = re.compile(r\"^▶ ([\\u3040-\\u309F]+)(?:「(.+)」|\\s?」)?\\s*([⓪\\u2460-\\u2470]+)(（[·、\\u4E00-\\u9FFFサ]+）)：(.*)$\")\n",
    "\"\"\"\n",
    "▶ あかい「赤い」 ②⓪（形）：红色的\n",
    "▶ あける「明ける」 ⓪（自一）：天亮了；过年\n",
    "▶ {{c1::あかい}}「赤い」 ②⓪（形）：{{c2::红色的}}\n",
    "\"\"\"\n",
    "for record in result:\n",
    "    result_type_answer.append({\n",
    "        \"word\": content_pattern.sub(r\"{{c1::\\1}}<br>\\2 \\3 \\4<br>{{c2::\\5}}\", record[\"example\"]),\n",
    "        \"word_explain\": record[\"example_explain\"],\n",
    "        \"kana_meaning\": record[\"meaning\"],\n",
    "        \"kana_meaning_explain\": record[\"explain\"],\n",
    "    })\n",
    "\n",
    "df_res = pd.DataFrame(result_type_answer, dtype=str).fillna(\"\")\n",
    "df_res.to_csv(\"word_roots(type_answer_anki).csv\", index=False, header=False)\n",
    "print(df_res.shape)\n",
    "df_res.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_md5(content):\n",
    "    content = content.encode(\"utf-8\")\n",
    "    md5_hash = hashlib.md5()\n",
    "    md5_hash.update(content)\n",
    "    return md5_hash.hexdigest()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "sentence_content_pattern = re.compile(r'([\\u2460-\\u2470])\\s*')\n",
    "invalid_line_pattern = re.compile(r'^[\\d\\s]+$')\n",
    "\n",
    "example_sentences = []\n",
    "groups = []\n",
    "with open(\"CS_Word_20240405_22.25.17.txt\", \"r\", encoding=\"utf-8\") as fp:\n",
    "    matched = False\n",
    "    sentence = None\n",
    "    last_line = \"\"\n",
    "    for line in fp.readlines():\n",
    "        line = line.strip()\n",
    "        if len(line) < 3 or invalid_line_pattern.match(line.strip()):\n",
    "            continue\n",
    "\n",
    "        if matched == True and last_line[-1] != \"。\":\n",
    "            sentence[\"content\"] += line\n",
    "            last_line = line\n",
    "            continue\n",
    "\n",
    "        if sentence_content_pattern.match(line.strip()):\n",
    "            matched = True\n",
    "            sentence = {\"content\": sentence_content_pattern.sub(r\"\\1 \", line)}\n",
    "        else:\n",
    "            if matched:\n",
    "                sentence[\"hiragana\"] = \"\"\n",
    "                sentence[\"meaning\"] = line\n",
    "                if sentence[\"meaning\"][1] == \" \":\n",
    "                    sentence[\"meaning\"] = sentence[\"meaning\"][2:]\n",
    "\n",
    "                if sentence[\"meaning\"][0] == \"囉\":\n",
    "                    sentence[\"meaning\"] = sentence[\"meaning\"][1:]\n",
    "\n",
    "                if sentence[\"content\"][0] == \"①\" and groups:\n",
    "                    example_sentences.append(groups)\n",
    "                    groups = []\n",
    "\n",
    "                groups.append(sentence)\n",
    "            matched = False\n",
    "        last_line = line\n",
    "\n",
    "if groups:\n",
    "    example_sentences.append(groups)\n",
    "\n",
    "print(example_sentences[1])\n",
    "with open(\"example_sentences.json\", \"w\", encoding=\"utf-8\") as fp:\n",
    "    fp.write(json.dumps(example_sentences, ensure_ascii=False, indent=4))\n",
    "    # pd.DataFrame(example_sentences, dtype=str).fillna(\"\").to_json(fp, orient=\"records\", force_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json(\"grammar_pd.json\", dtype=str)\n",
    "with open(\"grammar.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.loads(f.read())\n",
    "    df = pd.DataFrame(data)\n",
    "df.head()\n",
    "detail = df.loc[0].to_dict()\n",
    "print(type(detail[\"example\"]))\n",
    "print(detail[\"example\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from grammar_enumeration import grammars, df_grammars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grammars.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grammars.hiragana.str.replace(r\"[（）()\\s()]\", \"\").str.contains(r'なしに')[23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword = \"なくして(は)\"\n",
    "df = df_grammars\n",
    "df[df.content.str.replace(r\"[（）()\\s()]\", \"\").str.contains(keyword)\n",
    "                    | df.hiragana.str.replace(r\"[（）()\\s()]\", \"\").str.contains(keyword)\n",
    "                    | df.chinese_meaning.str.contains(keyword) | (df.source == keyword)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"其中N1级别201个，N2级别150个，N3级别140个，N4级别130个，N5级别120个。\"\n",
    "\n",
    "import json\n",
    "import re\n",
    "\n",
    "level_pattern = re.compile(r\"^N([1-5])文法\")\n",
    "unit_pattern = re.compile(r\"^第(\\d+)单元(.*)\")\n",
    "grammar_start_pattern = re.compile(r\"^\\d+\\. \")\n",
    "seq_pattern = re.compile(r\"^（\\d+）\")\n",
    "from collections import defaultdict\n",
    "\n",
    "lines = []\n",
    "part_keywords = {'接続': 391, '説明': 401, '例文': 910, '注意': 774, '接续': 308, '说明': 515, '读法': 4, '例词': 2, '补充': 1}\n",
    "level = 0\n",
    "current_grammar = \"\"\n",
    "current_part = \"\"\n",
    "current_seq = \"\"\n",
    "output = False\n",
    "# with open(\"日语蓝宝书.txt\", \"r\", encoding=\"utf-8\") as fp:\n",
    "# for idx, line in enumerate(fp.readlines()):\n",
    "content = \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "89. ～とて\n",
    "\n",
    "\n",
    "（1）\n",
    "\n",
    "接続\n",
    "\n",
    "名＋とて\n",
    "\n",
    "説明\n",
    "\n",
    "表示假定条件的逆接，“即使……也不例外”“即使是……也……”“甚至……”“就连……”。\n",
    "\n",
    "例文\n",
    "\n",
    "△常に冷静な彼とて やはり人間だから、感情的になってしまうこともあるのだろう。 【2009年12月真题】/尽管他平时很冷静，但是也有情绪化的时候吧 。\n",
    "\n",
    "△最近の電気製品は機能が多すぎる。開発者たちとて すべての機能が必要とは思わないのではないか。 【2007年真题】/最近的电器功能实在太多，就算是开发商也未必会认为所有的功能都有必要吧 。\n",
    "\n",
    "△私とて 試合に負けたことに悔しい。 /输掉了比赛，我也很懊恼 。\n",
    "\n",
    "注意\n",
    "\n",
    "①前面主要接续人名。\n",
    "\n",
    "②意思与「～としても」 相同，但「～とて」 是比较生硬的表达方式。\n",
    "\n",
    "（2）\n",
    "\n",
    "接続\n",
    "\n",
    "名＋だ/動た形＋とて\n",
    "\n",
    "説明\n",
    "\n",
    "表示假定条件的逆接，无论前项怎样，后项的原则都是一样的。“即使……也……”“即便……也……”“就算……也……”。\n",
    "\n",
    "例文\n",
    "\n",
    "△たとえ病気だとて 試験に欠席してはいけない。 /就算生病了也不能缺考 。\n",
    "\n",
    "△いくら 頼 たの んだとて 、できないことはできない。 /再怎么拜托，不能做的事情就是不能做 。\n",
    "\n",
    "△どんなに後悔したとて 、過ぎたことは今さらどうしようもない。 /就算再怎么后悔，对于已经过去的事情也没有办法 。\n",
    "\n",
    "注意\n",
    "\n",
    "常与「たとえ」 「どんなに」 「いくら」 等词一起使用。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "grammars = {}\n",
    "for _ in range(1):\n",
    "    for idx, line in enumerate(content.split(\"\\n\")):\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        if res := level_pattern.findall(line):\n",
    "            level = int(res[0])\n",
    "            # print(f\"Current Level: {level}\")\n",
    "            continue\n",
    "\n",
    "        if res := unit_pattern.findall(line):\n",
    "            # print(f\"Current Unit: {res[0][0]}.{res[0][1]}\")\n",
    "            continue\n",
    "\n",
    "        if grammar_start_pattern.match(line):\n",
    "            print(f\"Current Grammar: {line}\")\n",
    "            current_grammar = line\n",
    "            grammars[current_grammar] = {}\n",
    "            # current_part = \"\"\n",
    "            # current_seq = \"\"\n",
    "            continue\n",
    "\n",
    "        if seq_pattern.match(line):\n",
    "            # print(f\"Current Seq: {line}\")\n",
    "            current_seq = line\n",
    "            continue\n",
    "\n",
    "        lines.append(line)\n",
    "\n",
    "        if len(line) < 5 and line[:2] in part_keywords:\n",
    "            current_part = line[:2]\n",
    "            if current_part == \"例文\":\n",
    "                grammars[current_grammar][current_part] = []\n",
    "            else:\n",
    "                grammars[current_grammar][current_part] = \"\"\n",
    "            continue\n",
    "\n",
    "        if current_part == \"例文\":\n",
    "            grammars[current_grammar][current_part].append(line)\n",
    "        else:\n",
    "            grammars[current_grammar][current_part] += line + \"\\n\"\n",
    "\n",
    "print(json.dumps(grammars, ensure_ascii=False, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"其中N1级别201个，N2级别150个，N3级别140个，N4级别130个，N5级别120个。\"\n",
    "\n",
    "import json\n",
    "import re\n",
    "\n",
    "content = \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "89. ～とて\n",
    "\n",
    "\n",
    "（1）\n",
    "\n",
    "接続\n",
    "\n",
    "名＋とて\n",
    "\n",
    "説明\n",
    "\n",
    "表示假定条件的逆接，“即使……也不例外”“即使是……也……”“甚至……”“就连……”。\n",
    "\n",
    "例文\n",
    "\n",
    "△常に冷静な彼とて やはり人間だから、感情的になってしまうこともあるのだろう。 【2009年12月真题】/尽管他平时很冷静，但是也有情绪化的时候吧 。\n",
    "\n",
    "△最近の電気製品は機能が多すぎる。開発者たちとて すべての機能が必要とは思わないのではないか。 【2007年真题】/最近的电器功能实在太多，就算是开发商也未必会认为所有的功能都有必要吧 。\n",
    "\n",
    "△私とて 試合に負けたことに悔しい。 /输掉了比赛，我也很懊恼 。\n",
    "\n",
    "注意\n",
    "\n",
    "①前面主要接续人名。\n",
    "\n",
    "②意思与「～としても」 相同，但「～とて」 是比较生硬的表达方式。\n",
    "\n",
    "（2）\n",
    "\n",
    "接続\n",
    "\n",
    "名＋だ/動た形＋とて\n",
    "\n",
    "説明\n",
    "\n",
    "表示假定条件的逆接，无论前项怎样，后项的原则都是一样的。“即使……也……”“即便……也……”“就算……也……”。\n",
    "\n",
    "例文\n",
    "\n",
    "△たとえ病気だとて 試験に欠席してはいけない。 /就算生病了也不能缺考 。\n",
    "\n",
    "△いくら 頼 たの んだとて 、できないことはできない。 /再怎么拜托，不能做的事情就是不能做 。\n",
    "\n",
    "△どんなに後悔したとて 、過ぎたことは今さらどうしようもない。 /就算再怎么后悔，对于已经过去的事情也没有办法 。\n",
    "\n",
    "注意\n",
    "\n",
    "常与「たとえ」 「どんなに」 「いくら」 等词一起使用。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "level_pattern = re.compile(r\"^N([1-5])文法\")\n",
    "unit_pattern = re.compile(r\"^第(\\d+)单元(.*)\")\n",
    "grammar_start_pattern = re.compile(r\"^\\d+\\. \")\n",
    "seq_pattern = re.compile(r\"^（\\d+）\")\n",
    "\n",
    "\n",
    "def get_japanese_sequence_sign(number):\n",
    "    number = int(number)\n",
    "    if number <= 0:\n",
    "        return \"⓪\"\n",
    "\n",
    "    return chr(ord(\"\\u2460\") + number - 1)\n",
    "\n",
    "\n",
    "def extract_grammars(content):\n",
    "    grammars = {}\n",
    "    lines = []\n",
    "    part_keywords = {'接続': 391, '説明': 401, '例文': 910, '注意': 774, '接续': 308, '说明': 515, '读法': 4, '例词': 2, '补充': 1}\n",
    "    keyword_mapping = {\n",
    "        '接続': \"接续\",\n",
    "        '説明': \"说明\",\n",
    "        '读法': \"说明\",\n",
    "        '例词': \"说明\",\n",
    "        '补充': \"注意\",\n",
    "    }\n",
    "    level = 0\n",
    "    current_grammar = \"\"\n",
    "    current_part = \"\"\n",
    "    current_seq = \"\"\n",
    "    for idx, line in enumerate(content.split(\"\\n\")):\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        line = line.replace(\"真题】\", \"JLPT】\")\n",
    "        if res := level_pattern.findall(line):\n",
    "            level = int(res[0])\n",
    "            # print(f\"Current Level: {level}\")\n",
    "            continue\n",
    "\n",
    "        if res := unit_pattern.findall(line):\n",
    "            # print(f\"Current Unit: {res[0][0]}.{res[0][1]}\")\n",
    "            continue\n",
    "\n",
    "        if grammar_start_pattern.match(line):\n",
    "            # print(f\"Current Grammar: {line}\")\n",
    "            current_grammar = line\n",
    "            grammars[current_grammar] = {}\n",
    "            grammars[current_grammar][\"level\"] = level\n",
    "            # current_part = \"\"\n",
    "            # current_seq = \"\"\n",
    "            continue\n",
    "\n",
    "        if seq_pattern.match(line):\n",
    "            print(f\"Current Seq: {line}\")\n",
    "            current_seq = line\n",
    "            continue\n",
    "\n",
    "        lines.append(line)\n",
    "\n",
    "        if len(line) < 5 and line[:2] in part_keywords:\n",
    "            current_part = line[:2]\n",
    "            current_part = keyword_mapping.get(current_part, current_part)\n",
    "            index = f\"{get_japanese_sequence_sign(line[2])} \" if len(line) > 2 else \"\"\n",
    "            if current_part not in grammars[current_grammar]:\n",
    "                if current_part == \"例文\":\n",
    "                    grammars[current_grammar][current_part] = []\n",
    "                else:\n",
    "                    grammars[current_grammar][current_part] = \"\"\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            if current_part == \"例文\":\n",
    "                grammars[current_grammar][current_part].append(line)\n",
    "            else:\n",
    "                grammars[current_grammar][current_part] += index + line + \"\\n\"\n",
    "                index = \"\"\n",
    "        except Exception:\n",
    "            print((idx, line))\n",
    "            raise\n",
    "\n",
    "    return grammars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dumps(\"⓪①②③\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chr(ord(\"\\u2460\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kanji_with_kana_pattern = re.compile(r\"(?P<kanji>[\\u4E00-\\u9FFF]) (?P<kana>[\\u3040-\\u309F]+) \")\n",
    "content = \"中 なか 川 がわ 先 せん 生 せい 「あ、 山 やま 口 ぐち さん。 偶 ぐう 然 ぜん ですね。」\"\n",
    "content = \"辞书形 ⇒ 尊他语(辞书形)\"\n",
    "content = \"做 | する ⇒ なさる\"\n",
    "content = \"84. ～ 中 ちゅう/じゅう\"\n",
    "print(kanji_with_kana_pattern.findall(content))\n",
    "kanji_with_kana_pattern.sub(r\"<ruby>\\1<rt>\\2</rt></ruby>\", content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "content = \"△お 客 きゃく 様 さま 、このお 皿 さら をさげてもよろしいでしょうか 。 /客人您好，这个盘子我可以撤下去了吗 ？\"\n",
    "print(transform_content_to_ruby_rich_text(content))\n",
    "\n",
    "\n",
    "print(split_kanji_and_its_kana(content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_example_sentences(sentences):\n",
    "    new_sentences = []\n",
    "    for sentence in sentences:\n",
    "        if not sentence.startswith(\"△\"):\n",
    "            new_sentences[-1] += \"\\n\" + sentence\n",
    "        elif sentence.startswith(\"△B\"):\n",
    "            new_sentences[-1] += \"\\n  \" + sentence[1:]\n",
    "        else:\n",
    "            new_sentences.append(sentence)\n",
    "\n",
    "    return new_sentences\n",
    "\n",
    "\n",
    "with open(japanese_grammar.book_folder / \"日语蓝宝书.txt\", \"r\", encoding=\"utf-8\") as fp:\n",
    "    content = fp.read()\n",
    "\n",
    "empty_usage = []\n",
    "grammars = extract_grammars(content)\n",
    "# print(json.dumps(grammars, ensure_ascii=False, indent=4))\n",
    "part_keywords = {'接続': 391, '説明': 401, '例文': 910, '注意': 774, '接续': 308, '说明': 515, '读法': 4, '例词': 2, '补充': 1}\n",
    "example_sentence_pattern = re.compile(r\"^([^/]+?)(【\\d+年(?:\\d+月)?JLPT】)?/(.*)$\", re.DOTALL)\n",
    "header_pattern = re.compile(r\"^△\\s*\")\n",
    "formatted_grammars = []\n",
    "for idx, (grammar, detail) in enumerate(grammars.items()):\n",
    "    try:\n",
    "        content = grammar_start_pattern.sub(\"\", grammar)\n",
    "        hiragana = \"\"\n",
    "        rich_text = transform_content_to_ruby_rich_text(content)\n",
    "        if rich_text != content:\n",
    "            content, hiragana = split_kanji_and_its_kana(content)\n",
    "\n",
    "        example_sentences = detail.get(\"例文\") or []\n",
    "        example_sentences = merge_example_sentences(example_sentences)\n",
    "        formated_exmpale_sentences = []\n",
    "\n",
    "        for sentence_seq, sentence in enumerate(example_sentences):\n",
    "            sentence_content, tag, meaning = example_sentence_pattern.findall(sentence)[0]\n",
    "            sentence_content = header_pattern.sub(\"\", sentence_content)\n",
    "            sentence_content = f\"{get_japanese_sequence_sign(sentence_seq+1)} {sentence_content}\"\n",
    "\n",
    "            sentence_content = transform_content_to_ruby_rich_text(sentence_content)\n",
    "            formated_exmpale_sentences.append({\"content\": sentence_content, \"tag\": tag, \"meaning\": meaning})\n",
    "\n",
    "        # example_sentences = [{\"content\": sentence} for sentence in map(transform_content_to_ruby_rich_text, detail.get(\"例文\") or [])]\n",
    "\n",
    "        formatted_grammars.append({\n",
    "            \"id\": idx + 1,\n",
    "            \"content\": content,\n",
    "            \"hiragana\": hiragana,\n",
    "            \"meaning\": detail.get(\"说明\", \"\").strip(),\n",
    "            \"usage\": transform_content_to_ruby_rich_text(detail.get(\"接续\", \"\").strip()),\n",
    "            \"example\": formated_exmpale_sentences,\n",
    "            \"remark\": transform_content_to_ruby_rich_text(detail.get(\"注意\", \"\").strip()),\n",
    "            \"source\": \"S2N1-N5\",\n",
    "            \"japanese_meaning\": \"\",\n",
    "            \"chinese_meaning\": detail.get(\"说明\", \"\").strip(),\n",
    "            \"level\": detail[\"level\"]\n",
    "        })\n",
    "        if formatted_grammars[-1][\"usage\"] and formatted_grammars[-1][\"usage\"] != transform_content_to_ruby_rich_text(formatted_grammars[-1][\"usage\"]):\n",
    "            empty_usage.append(formatted_grammars[-1])\n",
    "    except Exception:\n",
    "        print(grammar)\n",
    "        print(content)\n",
    "        print(detail)\n",
    "        raise\n",
    "\n",
    "print(formatted_grammars[0])\n",
    "with open(\"grammars_N1-N5.json\", \"w\", encoding=\"utf-8\") as fp:\n",
    "    fp.write(json.dumps(formatted_grammars, ensure_ascii=False, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for detail in empty_usage:\n",
    "    print(detail[\"usage\"])\n",
    "    print(transform_content_to_ruby_rich_text(detail[\"usage\"]))\n",
    "    print(\"=\" * 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 日语平假名unicode编码范围：\\u3040-\\u309F\n",
    "# 日语片假名unicode编码范围：\\u30A0-\\u30FF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = \"私の料理を一口食べるなり 、父は変な顔をして席を立ってしまった。\"\n",
    "get_md5(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from zipfile import ZipFile\n",
    "\n",
    "\n",
    "def extract_epub_images(epub_path, output_folder):\n",
    "    with ZipFile(epub_path, 'r') as epub_file:\n",
    "        for file_name in epub_file.namelist():\n",
    "            if file_name.endswith(('.jpg', '.jpeg', '.png', '.gif')):\n",
    "                output_file_path = os.path.join(output_folder, os.path.basename(file_name))\n",
    "                with open(output_file_path, 'wb') as output_file:\n",
    "                    output_file.write(epub_file.read(file_name))\n",
    "\n",
    "    print(\"Image extraction completed.\")\n",
    "\n",
    "\n",
    "# 示例用法\n",
    "epub_path = japanese_grammar.book_folder  / \"超值白金版.蓝宝书大全集：新日本语能力考试N1-N5文法详解（最新修订版） (许小明) (Z-Library).epub\" # 替换为实际的EPUB文件路径\n",
    "output_folder = japanese_grammar.book_folder / \"temp\"  # 替换为实际的输出文件夹路径\n",
    "\n",
    "extract_epub_images(epub_path, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fpdf import FPDF\n",
    "import os\n",
    "\n",
    "\n",
    "def merge_jpg_to_pdf(folder_path, output_path):\n",
    "    pdf = FPDF()\n",
    "\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith(\".jpg\"):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            pdf.add_page()\n",
    "            pdf.image(file_path, x=0, y=0, w=210)  # 设置图片大小为A4纸尺寸\n",
    "\n",
    "    pdf.output(output_path, \"F\")\n",
    "    print(\"PDF merging completed.\")\n",
    "\n",
    "\n",
    "# 示例用法\n",
    "folder_path = japanese_grammar.book_folder / \"temp\"  # 替换为实际的输出文件夹路径\n",
    "output_path = japanese_grammar.book_folder / \"file.pdf\"  # 替换为实际的输出文件夹路径\n",
    "\n",
    "merge_jpg_to_pdf(folder_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install fpdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import sys\n",
    "\n",
    "kana_meaning_pattern = re.compile(r\"^[△\\u2460-\\u2470\\s]*\")\n",
    "rich_text_pattern = re.compile(r\"<ruby>(.*?)<rt>.*?</rt></ruby>\")\n",
    "RE_WHITESPACES_PATTERN = re.compile(r\"\\s+\")\n",
    "\n",
    "with open(\"grammars_N1-N5.json\", \"r\", encoding=\"utf-8\") as fp:\n",
    "    formatted_grammars = json.loads(fp.read())\n",
    "\n",
    "total_sentence_amount = sum([len(grammar[\"example\"]) for grammar in formatted_grammars])\n",
    "print(f\"Total sentence amount: {total_sentence_amount}\")\n",
    "\n",
    "for grammar in tqdm(formatted_grammars, file=sys.stdout):\n",
    "    for sentence in grammar[\"example\"]:\n",
    "        content = sentence[\"content\"]\n",
    "        content = kana_meaning_pattern.sub(\"\", content)\n",
    "        content = rich_text_pattern.sub(r\"\\1\", content)\n",
    "        content = RE_WHITESPACES_PATTERN.sub(\"\", content)\n",
    "\n",
    "        content_md5 = get_md5(content)\n",
    "        audio_path = f\"japanese_grammar/audio/{content_md5}.mp3\"\n",
    "\n",
    "        text_to_audio.generate_audio([\n",
    "            (None, content),\n",
    "        ], audio_path, log=tqdm.write)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar_content_pattern = re.compile(r\"^\\d? ?[~\\u3040-\\u309F\\u30A0-\\u30FF\\u4E00-\\u9FFF  ()]+\")\n",
    "\n",
    "grammar_content_pattern.findall(\"~ 際 (に)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = \"ここに車を止められるのは、許可をもらっている人（a だけ　b に限り）です。\"\n",
    "content = re.sub(\"\\s+\", \" \", content)\n",
    "print(content)\n",
    "\n",
    "question_pattern = re.compile(r\"[（(](?:[\\da-n]\\s*[~\\u3040-\\u309F\\u30A0-\\u30FF\\u4E00-\\u9FFF]+\\s*)+[)）]\")\n",
    "print(question_pattern.findall(content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(r\"(?:[\\da-n]\\s*[~\\u3040-\\u309F\\u30A0-\\u30FF\\u4E00-\\u9FFF]+\\s*)+\", \"a だけ b に限り\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_pattern = re.compile(r\"[(（](?:[\\da-n]\\s*[~\\u3040-\\u309F\\u30A0-\\u30FF\\u4E00-\\u9FFF]+\\s*)+[）)]*\\?\")\n",
    "print(question_pattern.findall(\"こちらの会議室をご利用になる際は、 受付で必要事項をご記入ください。\"))\n",
    "print(question_pattern.findall(\"ここに車を止められるのは、許可をもらっている人（a だけ　b に限り）です。\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "grammar_content_pattern = re.compile(r\"^\\d [~\\u3040-\\u309F\\u30A0-\\u30FF\\u4E00-\\u9FFF  ()]+\")\n",
    "sentence_content_pattern = re.compile(r'([\\u2460-\\u2470])\\s*')\n",
    "invalid_line_pattern = re.compile(r'^[\\d\\s]+$')\n",
    "\n",
    "example_sentences = []\n",
    "with open(\"CS_Word_20240403_23.41.17.txt\", \"r\", encoding=\"utf-8\") as fp:\n",
    "    matched = False\n",
    "    sentence = None\n",
    "    last_line = \"\"\n",
    "    for line in fp.readlines():\n",
    "        line = line.strip()\n",
    "        if len(line) < 3 or invalid_line_pattern.match(line.strip()):\n",
    "            continue\n",
    "\n",
    "        if grammar_content_pattern.match(line):\n",
    "            print(line)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
