{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import hashlib\n",
    "import time\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import sys\n",
    "from config import japanese_grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import text_to_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chinese_to_number(chinese_num):\n",
    "    num_dict = {\n",
    "        '零': 0, '一': 1, '二': 2, '三': 3, '四': 4,\n",
    "        '五': 5, '六': 6, '七': 7, '八': 8, '九': 9\n",
    "    }\n",
    "\n",
    "    unit_dict = {\n",
    "        '十': 10, '百': 100, '千': 1000\n",
    "    }\n",
    "\n",
    "    result = 0\n",
    "    unit = 1  # 当前的单位，初始为1\n",
    "    num = 0  # 当前的数字，初始为0\n",
    "    length = len(chinese_num)\n",
    "\n",
    "    for i in range(length):\n",
    "        char = chinese_num[i]\n",
    "\n",
    "        if char in num_dict:\n",
    "            num = num_dict[char]\n",
    "\n",
    "            if i == length - 1:\n",
    "                result += num * unit\n",
    "\n",
    "        elif char in unit_dict:\n",
    "            unit = unit_dict[char]\n",
    "\n",
    "            if i == 0 or chinese_num[i - 1] not in num_dict:\n",
    "                num = 1\n",
    "\n",
    "            result += num * unit\n",
    "            unit = 1\n",
    "            num = 0\n",
    "\n",
    "    return result\n",
    "\n",
    "# 测试样例\n",
    "print(chinese_to_number(\"一百二十九\"))  # 129\n",
    "print(chinese_to_number(\"九百九十九\"))  # 999\n",
    "print(chinese_to_number(\"五十三\"))      # 53\n",
    "print(chinese_to_number(\"五\"))          # 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_md5(content):\n",
    "    content = content.encode(\"utf-8\")\n",
    "    md5_hash = hashlib.md5()\n",
    "    md5_hash.update(content)\n",
    "    return md5_hash.hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kanji_with_kana_pattern = re.compile(\n",
    "    r\"\\s*(?P<kanji>[\\u4E00-\\u9FFF]) (?P<kana>[\\u3040-\\u309F]+)\\s*\"\n",
    ")\n",
    "\n",
    "\n",
    "def transform_content_to_ruby_rich_text(content, pattern=None):\n",
    "    if pattern is None:\n",
    "        pattern = kanji_with_kana_pattern\n",
    "\n",
    "    return pattern.sub(r\"<ruby>\\1<rt>\\2</rt></ruby>\", content)\n",
    "\n",
    "\n",
    "def split_kanji_and_its_kana(content, pattern=None):\n",
    "    if pattern is None:\n",
    "        pattern = kanji_with_kana_pattern\n",
    "\n",
    "    return kanji_with_kana_pattern.sub(r\"\\1\", content), kanji_with_kana_pattern.sub(\n",
    "        r\"\\2\", content\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"其中N1级别201个，N2级别150个，N3级别140个，N4级别130个，N5级别120个。\"\n",
    "\n",
    "import json\n",
    "import re\n",
    "\n",
    "content = \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "89. ～とて\n",
    "\n",
    "\n",
    "（1）\n",
    "\n",
    "接続\n",
    "\n",
    "名＋とて\n",
    "\n",
    "説明\n",
    "\n",
    "表示假定条件的逆接，“即使……也不例外”“即使是……也……”“甚至……”“就连……”。\n",
    "\n",
    "例文\n",
    "\n",
    "△常に冷静な彼とて やはり人間だから、感情的になってしまうこともあるのだろう。 【2009年12月真题】/尽管他平时很冷静，但是也有情绪化的时候吧 。\n",
    "\n",
    "△最近の電気製品は機能が多すぎる。開発者たちとて すべての機能が必要とは思わないのではないか。 【2007年真题】/最近的电器功能实在太多，就算是开发商也未必会认为所有的功能都有必要吧 。\n",
    "\n",
    "△私とて 試合に負けたことに悔しい。 /输掉了比赛，我也很懊恼 。\n",
    "\n",
    "注意\n",
    "\n",
    "①前面主要接续人名。\n",
    "\n",
    "②意思与「～としても」 相同，但「～とて」 是比较生硬的表达方式。\n",
    "\n",
    "（2）\n",
    "\n",
    "接続\n",
    "\n",
    "名＋だ/動た形＋とて\n",
    "\n",
    "説明\n",
    "\n",
    "表示假定条件的逆接，无论前项怎样，后项的原则都是一样的。“即使……也……”“即便……也……”“就算……也……”。\n",
    "\n",
    "例文\n",
    "\n",
    "△たとえ病気だとて 試験に欠席してはいけない。 /就算生病了也不能缺考 。\n",
    "\n",
    "△いくら 頼 たの んだとて 、できないことはできない。 /再怎么拜托，不能做的事情就是不能做 。\n",
    "\n",
    "△どんなに後悔したとて 、過ぎたことは今さらどうしようもない。 /就算再怎么后悔，对于已经过去的事情也没有办法 。\n",
    "\n",
    "注意\n",
    "\n",
    "常与「たとえ」 「どんなに」 「いくら」 等词一起使用。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "level_pattern = re.compile(r\"^N([1-5])文法\")\n",
    "unit_pattern = re.compile(r\"^第(\\d+)单元(.*)\")\n",
    "grammar_start_pattern = re.compile(r\"^\\d+\\. \")\n",
    "seq_pattern = re.compile(r\"^（\\d+）\")\n",
    "\n",
    "\n",
    "def get_japanese_sequence_sign(number):\n",
    "    number = int(number)\n",
    "    if number <= 0:\n",
    "        return \"⓪\"\n",
    "\n",
    "    return chr(ord(\"\\u2460\") + number - 1)\n",
    "\n",
    "def merge_example_sentences(sentences):\n",
    "    new_sentences = []\n",
    "    for sentence in sentences:\n",
    "        if not sentence.startswith(\"△\"):\n",
    "            new_sentences[-1] += \"\\n\" + sentence\n",
    "        elif sentence.startswith(\"△B\"):\n",
    "            new_sentences[-1] += \"\\n  \" + sentence[1:]\n",
    "        else:\n",
    "            new_sentences.append(sentence)\n",
    "\n",
    "    return new_sentences\n",
    "\n",
    "\n",
    "def extract_grammars(content):\n",
    "    grammars = {}\n",
    "    lines = []\n",
    "    part_keywords = {'接続': 391, '説明': 401, '例文': 910, '注意': 774, '接续': 308, '说明': 515, '读法': 4, '例词': 2, '补充': 1}\n",
    "    keyword_mapping = {\n",
    "        '接続': \"接续\",\n",
    "        '説明': \"说明\",\n",
    "        '读法': \"说明\",\n",
    "        '例词': \"说明\",\n",
    "        '补充': \"注意\",\n",
    "    }\n",
    "    level = 0\n",
    "    current_grammar = \"\"\n",
    "    current_part = \"\"\n",
    "    current_seq = \"\"\n",
    "    for idx, line in enumerate(content.split(\"\\n\")):\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        line = line.replace(\"真题】\", \"JLPT】\")\n",
    "        if res := level_pattern.findall(line):\n",
    "            level = int(res[0])\n",
    "            # print(f\"Current Level: {level}\")\n",
    "            continue\n",
    "\n",
    "        if res := unit_pattern.findall(line):\n",
    "            # print(f\"Current Unit: {res[0][0]}.{res[0][1]}\")\n",
    "            continue\n",
    "\n",
    "        if grammar_start_pattern.match(line):\n",
    "            # print(f\"Current Grammar: {line}\")\n",
    "            current_grammar = line\n",
    "            grammars[current_grammar] = {}\n",
    "            grammars[current_grammar][\"level\"] = level\n",
    "            # current_part = \"\"\n",
    "            # current_seq = \"\"\n",
    "            continue\n",
    "\n",
    "        if seq_pattern.match(line):\n",
    "            print(f\"Current Seq: {line}\")\n",
    "            current_seq = line\n",
    "            continue\n",
    "\n",
    "        lines.append(line)\n",
    "\n",
    "        if len(line) < 5 and line[:2] in part_keywords:\n",
    "            current_part = line[:2]\n",
    "            current_part = keyword_mapping.get(current_part, current_part)\n",
    "            index = f\"{get_japanese_sequence_sign(line[2])} \" if len(line) > 2 else \"\"\n",
    "            if current_part not in grammars[current_grammar]:\n",
    "                if current_part == \"例文\":\n",
    "                    grammars[current_grammar][current_part] = []\n",
    "                else:\n",
    "                    grammars[current_grammar][current_part] = \"\"\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            if current_part == \"例文\":\n",
    "                grammars[current_grammar][current_part].append(line)\n",
    "            else:\n",
    "                grammars[current_grammar][current_part] += index + line + \"\\n\"\n",
    "                index = \"\"\n",
    "        except Exception:\n",
    "            print((idx, line))\n",
    "            raise\n",
    "\n",
    "    return grammars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"grammars_N1-N5.json\", \"r\", encoding=\"utf-8\") as fp:\n",
    "    df = pd.DataFrame(json.loads(fp.read()))\n",
    "\n",
    "\n",
    "df_res = df\n",
    "keyword = \"に\"\n",
    "for item in keyword.split():\n",
    "    df_res = df_res[df_res.content.str.replace(r\"[（）()\\s]\", \"\", regex=True).str.contains(item)\n",
    "                    | df_res.hiragana.str.replace(r\"[（）()\\s]\", \"\", regex=True).str.contains(item)\n",
    "                    | df_res.chinese_meaning.str.contains(item) | (df_res.source == item)]\n",
    "df_res[\"order\"] = df_res.content.apply(len)\n",
    "df_res = df_res.sort_values(by=\"order\")\n",
    "[df.loc[index].to_dict() for index in df_res.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('grammars_N1-N5.json').fillna(\"\")\n",
    "df.level = df.level.apply(lambda x: f\"N{x}\")\n",
    "df.example = df.example.apply(lambda example: \"<br>\".join([f\"{sentence['content']}{sentence['tag']}<br>{sentence['meaning']}<br>\" for sentence in example]))\n",
    "print(df.columns)\n",
    "print(df.level.unique())\n",
    "print(df.source.unique())\n",
    "df = df[[\"content\", \"hiragana\", \"meaning\", \"usage\", \"remark\", \"level\", \"example\"]]\n",
    "df.to_csv(\"grammars_N1-N5_anki.csv\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kanji_with_kana_pattern = re.compile(\n",
    "    r\"(?P<kanji>[\\u4E00-\\u9FFF])(?P<kana>[\\u3040-\\u309F]+)(?:\\s|$)\"\n",
    ")\n",
    "content = \"\"\"\n",
    "▶ あかい「赤い」 ②⓪（形）：红色的\n",
    "\n",
    "解 古代日本人不区分“红”和“亮”两个概念，因为明亮的太阳光和火光都偏红色。\n",
    "\n",
    "例 赤あか い花はな が咲さ いている。/红色的花开了。\n",
    "\"\"\"\n",
    "\n",
    "print(transform_content_to_ruby_rich_text(content, pattern=kanji_with_kana_pattern))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kana_meaning_pattern = re.compile(r\"^\\d{2,}\\s\")\n",
    "example_pattern = re.compile(r\"^▶\")\n",
    "\n",
    "result = []\n",
    "kanji_with_kana_pattern = re.compile(\n",
    "    r\"(?P<kanji>[\\u4E00-\\u9FFF]+)(?P<kana>[\\u3040-\\u309F]+)(?:\\s|$)\"\n",
    ")\n",
    "file_path = (\n",
    "    japanese_grammar.book_folder\n",
    "    / \"记单词，一定要学的130个日语词根-_张铭_-_Z-Library_.txt\"\n",
    ")\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as fp:\n",
    "    current_kana = \"\"\n",
    "    current_meaning_item = \"\"\n",
    "    current_meanning_explain = []\n",
    "    current_example = \"\"\n",
    "    current_example_explain = []\n",
    "    within_example_part = False\n",
    "\n",
    "    lines = fp.readlines()\n",
    "    last_line = \"\"\n",
    "    for line in lines:\n",
    "        content = line.strip()\n",
    "        if not content:\n",
    "            continue\n",
    "\n",
    "        if current_kana and content.startswith(\"第二章\"):\n",
    "            break\n",
    "\n",
    "        # 假名\n",
    "        if len(content) == 1:\n",
    "            current_kana = content\n",
    "            current_meaning_item = \"\"\n",
    "            current_meanning_explain = []\n",
    "            current_example = \"\"\n",
    "            current_example_explain = []\n",
    "            within_example_part = False\n",
    "            continue\n",
    "\n",
    "        # 假名词根含义\n",
    "        if current_kana and kana_meaning_pattern.match(content):\n",
    "            if current_example:\n",
    "                result.append(\n",
    "                    {\n",
    "                        \"kana\": current_kana,\n",
    "                        \"meaning\": current_meaning_item,\n",
    "                        \"explain\": \"<br>\".join(current_meanning_explain),\n",
    "                        \"example\": current_example,\n",
    "                        \"example_explain\": \"<br>\".join(\n",
    "                            [\n",
    "                                transform_content_to_ruby_rich_text(\n",
    "                                    content, pattern=kanji_with_kana_pattern\n",
    "                                )\n",
    "                                for content in current_example_explain\n",
    "                            ]\n",
    "                        ),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            current_meaning_item = content\n",
    "            current_meanning_explain = []\n",
    "            current_example = \"\"\n",
    "            within_example_part = False\n",
    "            item_idx = content.split()[0]\n",
    "            continue\n",
    "\n",
    "        # 词根含义举例\n",
    "        if current_kana and example_pattern.match(content):\n",
    "            within_example_part = True\n",
    "            if current_example:\n",
    "                result.append(\n",
    "                    {\n",
    "                        \"kana\": current_kana,\n",
    "                        \"meaning\": current_meaning_item,\n",
    "                        \"explain\": \"<br>\".join(current_meanning_explain),\n",
    "                        \"example\": current_example,\n",
    "                        \"example_explain\": \"<br>\".join(\n",
    "                            [\n",
    "                                transform_content_to_ruby_rich_text(\n",
    "                                    content, pattern=kanji_with_kana_pattern\n",
    "                                )\n",
    "                                for content in current_example_explain\n",
    "                            ]\n",
    "                        ),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            current_example = content\n",
    "            current_example_explain = []\n",
    "            continue\n",
    "\n",
    "        # 词根举例 解释\n",
    "        if current_example:\n",
    "            current_example_explain.append(content)\n",
    "            continue\n",
    "\n",
    "        # 假名词根 解释\n",
    "        if current_kana and not within_example_part:\n",
    "            current_meanning_explain.append(content)\n",
    "\n",
    "        last_line = content\n",
    "\n",
    "\n",
    "if current_example_explain:\n",
    "    result.append(\n",
    "        {\n",
    "            \"kana\": current_kana,\n",
    "            \"meaning\": current_meaning_item,\n",
    "            \"explain\": \"<br>\".join(current_meanning_explain),\n",
    "            \"example\": current_example,\n",
    "            \"example_explain\": \"<br>\".join(\n",
    "                [\n",
    "                    transform_content_to_ruby_rich_text(\n",
    "                        content, pattern=kanji_with_kana_pattern\n",
    "                    )\n",
    "                    for content in current_example_explain\n",
    "                ]\n",
    "            ),\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for record in result:\n",
    "    if record[\"example\"] == \"▶ あか「垢」 ②（名）：污垢；水锈\":\n",
    "        print(record)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(result).fillna(\"\")\n",
    "print(df.shape)\n",
    "df.to_csv(\"word_roots.csv\", index=False)\n",
    "df.to_csv(\"word_roots_anki.csv\", index=False, header=False)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_kana = \"\"\n",
    "last_meaning = \"\"\n",
    "for index in df.index:\n",
    "    detail = df.loc[index].to_dict()\n",
    "    if detail[\"kana\"] != last_kana:\n",
    "        print(\"=\" * 32)\n",
    "        print(detail[\"kana\"])\n",
    "    if detail[\"meaning\"] != last_meaning:\n",
    "        print()\n",
    "        print(detail[\"meaning\"])\n",
    "        print(detail[\"explain\"])\n",
    "\n",
    "    example_explain = detail[\"example_explain\"].split(\"<br>\")[0]\n",
    "    example_explain = \"\" if example_explain[0] != \"解\" else \" \".join(example_explain.split()[1:])\n",
    "    print(detail[\"example\"])\n",
    "    if example_explain:\n",
    "        print(example_explain)\n",
    "\n",
    "    last_kana = detail[\"kana\"]\n",
    "    last_meaning = detail[\"meaning\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_type_answer = []\n",
    "\n",
    "kana_pattern = re.compile(r\"(?<=^▶ )([\\u3040-\\u309F]+)(?=「)\")\n",
    "meaning_pattern = re.compile(r\"(?<=：)(.+)$\")\n",
    "content_pattern = re.compile(r\"^▶ ([\\u3040-\\u309F]+)(?:「(.+)」|\\s?」)?\\s*([⓪\\u2460-\\u2470]+)(（[·、\\u4E00-\\u9FFFサ]+）)：(.*)$\")\n",
    "\"\"\"\n",
    "▶ あかい「赤い」 ②⓪（形）：红色的\n",
    "▶ あける「明ける」 ⓪（自一）：天亮了；过年\n",
    "▶ {{c1::あかい}}「赤い」 ②⓪（形）：{{c2::红色的}}\n",
    "\"\"\"\n",
    "for record in result:\n",
    "    result_type_answer.append({\n",
    "        \"word\": content_pattern.sub(r\"{{c1::\\1}}<br>\\2 \\3 \\4<br>{{c2::\\5}}\", record[\"example\"]),\n",
    "        \"word_explain\": record[\"example_explain\"],\n",
    "        \"kana_meaning\": record[\"meaning\"],\n",
    "        \"kana_meaning_explain\": record[\"explain\"],\n",
    "    })\n",
    "\n",
    "df_res = pd.DataFrame(result_type_answer, dtype=str).fillna(\"\")\n",
    "df_res.to_csv(\"word_roots(type_answer_anki).csv\", index=False, header=False)\n",
    "print(df_res.shape)\n",
    "df_res.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "sentence_content_pattern = re.compile(r'([\\u2460-\\u2470])\\s*')\n",
    "invalid_line_pattern = re.compile(r'^[\\d\\s]+$')\n",
    "\n",
    "example_sentences = []\n",
    "groups = []\n",
    "with open(\"CS_Word_20240405_22.25.17.txt\", \"r\", encoding=\"utf-8\") as fp:\n",
    "    matched = False\n",
    "    sentence = None\n",
    "    last_line = \"\"\n",
    "    for line in fp.readlines():\n",
    "        line = line.strip()\n",
    "        if len(line) < 3 or invalid_line_pattern.match(line.strip()):\n",
    "            continue\n",
    "\n",
    "        if matched == True and last_line[-1] != \"。\":\n",
    "            sentence[\"content\"] += line\n",
    "            last_line = line\n",
    "            continue\n",
    "\n",
    "        if sentence_content_pattern.match(line.strip()):\n",
    "            matched = True\n",
    "            sentence = {\"content\": sentence_content_pattern.sub(r\"\\1 \", line)}\n",
    "        else:\n",
    "            if matched:\n",
    "                sentence[\"hiragana\"] = \"\"\n",
    "                sentence[\"meaning\"] = line\n",
    "                if sentence[\"meaning\"][1] == \" \":\n",
    "                    sentence[\"meaning\"] = sentence[\"meaning\"][2:]\n",
    "\n",
    "                if sentence[\"meaning\"][0] == \"囉\":\n",
    "                    sentence[\"meaning\"] = sentence[\"meaning\"][1:]\n",
    "\n",
    "                if sentence[\"content\"][0] == \"①\" and groups:\n",
    "                    example_sentences.append(groups)\n",
    "                    groups = []\n",
    "\n",
    "                groups.append(sentence)\n",
    "            matched = False\n",
    "        last_line = line\n",
    "\n",
    "if groups:\n",
    "    example_sentences.append(groups)\n",
    "\n",
    "print(example_sentences[1])\n",
    "with open(\"example_sentences.json\", \"w\", encoding=\"utf-8\") as fp:\n",
    "    fp.write(json.dumps(example_sentences, ensure_ascii=False, indent=4))\n",
    "    # pd.DataFrame(example_sentences, dtype=str).fillna(\"\").to_json(fp, orient=\"records\", force_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json(\"grammar_pd.json\", dtype=str)\n",
    "with open(\"grammar.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.loads(f.read())\n",
    "    df = pd.DataFrame(data)\n",
    "df.head()\n",
    "detail = df.loc[0].to_dict()\n",
    "print(type(detail[\"example\"]))\n",
    "print(detail[\"example\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from grammar_enumeration import grammars, df_grammars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grammars.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grammars.hiragana.str.replace(r\"[（）()\\s()]\", \"\").str.contains(r'なしに')[23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword = \"なくして(は)\"\n",
    "df = df_grammars\n",
    "df[df.content.str.replace(r\"[（）()\\s()]\", \"\").str.contains(keyword)\n",
    "                    | df.hiragana.str.replace(r\"[（）()\\s()]\", \"\").str.contains(keyword)\n",
    "                    | df.chinese_meaning.str.contains(keyword) | (df.source == keyword)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"其中N1级别201个，N2级别150个，N3级别140个，N4级别130个，N5级别120个。\"\n",
    "\n",
    "import json\n",
    "import re\n",
    "\n",
    "level_pattern = re.compile(r\"^N([1-5])文法\")\n",
    "unit_pattern = re.compile(r\"^第(\\d+)单元(.*)\")\n",
    "grammar_start_pattern = re.compile(r\"^\\d+\\. \")\n",
    "seq_pattern = re.compile(r\"^（\\d+）\")\n",
    "from collections import defaultdict\n",
    "\n",
    "lines = []\n",
    "part_keywords = {'接続': 391, '説明': 401, '例文': 910, '注意': 774, '接续': 308, '说明': 515, '读法': 4, '例词': 2, '补充': 1}\n",
    "level = 0\n",
    "current_grammar = \"\"\n",
    "current_part = \"\"\n",
    "current_seq = \"\"\n",
    "output = False\n",
    "# with open(\"日语蓝宝书.txt\", \"r\", encoding=\"utf-8\") as fp:\n",
    "# for idx, line in enumerate(fp.readlines()):\n",
    "content = \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "89. ～とて\n",
    "\n",
    "\n",
    "（1）\n",
    "\n",
    "接続\n",
    "\n",
    "名＋とて\n",
    "\n",
    "説明\n",
    "\n",
    "表示假定条件的逆接，“即使……也不例外”“即使是……也……”“甚至……”“就连……”。\n",
    "\n",
    "例文\n",
    "\n",
    "△常に冷静な彼とて やはり人間だから、感情的になってしまうこともあるのだろう。 【2009年12月真题】/尽管他平时很冷静，但是也有情绪化的时候吧 。\n",
    "\n",
    "△最近の電気製品は機能が多すぎる。開発者たちとて すべての機能が必要とは思わないのではないか。 【2007年真题】/最近的电器功能实在太多，就算是开发商也未必会认为所有的功能都有必要吧 。\n",
    "\n",
    "△私とて 試合に負けたことに悔しい。 /输掉了比赛，我也很懊恼 。\n",
    "\n",
    "注意\n",
    "\n",
    "①前面主要接续人名。\n",
    "\n",
    "②意思与「～としても」 相同，但「～とて」 是比较生硬的表达方式。\n",
    "\n",
    "（2）\n",
    "\n",
    "接続\n",
    "\n",
    "名＋だ/動た形＋とて\n",
    "\n",
    "説明\n",
    "\n",
    "表示假定条件的逆接，无论前项怎样，后项的原则都是一样的。“即使……也……”“即便……也……”“就算……也……”。\n",
    "\n",
    "例文\n",
    "\n",
    "△たとえ病気だとて 試験に欠席してはいけない。 /就算生病了也不能缺考 。\n",
    "\n",
    "△いくら 頼 たの んだとて 、できないことはできない。 /再怎么拜托，不能做的事情就是不能做 。\n",
    "\n",
    "△どんなに後悔したとて 、過ぎたことは今さらどうしようもない。 /就算再怎么后悔，对于已经过去的事情也没有办法 。\n",
    "\n",
    "注意\n",
    "\n",
    "常与「たとえ」 「どんなに」 「いくら」 等词一起使用。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "grammars = {}\n",
    "for _ in range(1):\n",
    "    for idx, line in enumerate(content.split(\"\\n\")):\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        if res := level_pattern.findall(line):\n",
    "            level = int(res[0])\n",
    "            # print(f\"Current Level: {level}\")\n",
    "            continue\n",
    "\n",
    "        if res := unit_pattern.findall(line):\n",
    "            # print(f\"Current Unit: {res[0][0]}.{res[0][1]}\")\n",
    "            continue\n",
    "\n",
    "        if grammar_start_pattern.match(line):\n",
    "            print(f\"Current Grammar: {line}\")\n",
    "            current_grammar = line\n",
    "            grammars[current_grammar] = {}\n",
    "            # current_part = \"\"\n",
    "            # current_seq = \"\"\n",
    "            continue\n",
    "\n",
    "        if seq_pattern.match(line):\n",
    "            # print(f\"Current Seq: {line}\")\n",
    "            current_seq = line\n",
    "            continue\n",
    "\n",
    "        lines.append(line)\n",
    "\n",
    "        if len(line) < 5 and line[:2] in part_keywords:\n",
    "            current_part = line[:2]\n",
    "            if current_part == \"例文\":\n",
    "                grammars[current_grammar][current_part] = []\n",
    "            else:\n",
    "                grammars[current_grammar][current_part] = \"\"\n",
    "            continue\n",
    "\n",
    "        if current_part == \"例文\":\n",
    "            grammars[current_grammar][current_part].append(line)\n",
    "        else:\n",
    "            grammars[current_grammar][current_part] += line + \"\\n\"\n",
    "\n",
    "print(json.dumps(grammars, ensure_ascii=False, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dumps(\"⓪①②③\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chr(ord(\"\\u2460\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kanji_with_kana_pattern = re.compile(r\"(?P<kanji>[\\u4E00-\\u9FFF]) (?P<kana>[\\u3040-\\u309F]+) \")\n",
    "content = \"中 なか 川 がわ 先 せん 生 せい 「あ、 山 やま 口 ぐち さん。 偶 ぐう 然 ぜん ですね。」\"\n",
    "content = \"辞书形 ⇒ 尊他语(辞书形)\"\n",
    "content = \"做 | する ⇒ なさる\"\n",
    "content = \"84. ～ 中 ちゅう/じゅう\"\n",
    "print(kanji_with_kana_pattern.findall(content))\n",
    "kanji_with_kana_pattern.sub(r\"<ruby>\\1<rt>\\2</rt></ruby>\", content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "content = \"△お 客 きゃく 様 さま 、このお 皿 さら をさげてもよろしいでしょうか 。 /客人您好，这个盘子我可以撤下去了吗 ？\"\n",
    "print(transform_content_to_ruby_rich_text(content))\n",
    "\n",
    "\n",
    "print(split_kanji_and_its_kana(content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(japanese_grammar.book_folder / \"日语蓝宝书.txt\", \"r\", encoding=\"utf-8\") as fp:\n",
    "    content = fp.read()\n",
    "\n",
    "empty_usage = []\n",
    "grammars = extract_grammars(content)\n",
    "\n",
    "len(grammars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(grammars.values())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(japanese_grammar.book_folder / \"日语蓝宝书.txt\", \"r\", encoding=\"utf-8\") as fp:\n",
    "    content = fp.read()\n",
    "\n",
    "empty_usage = []\n",
    "grammars = extract_grammars(content)\n",
    "# print(json.dumps(grammars, ensure_ascii=False, indent=4))\n",
    "part_keywords = {'接続': 391, '説明': 401, '例文': 910, '注意': 774, '接续': 308, '说明': 515, '读法': 4, '例词': 2, '补充': 1}\n",
    "example_sentence_pattern = re.compile(r\"^([^/]+?)(【\\d+年(?:\\d+月)?JLPT】)?/(.*)$\", re.DOTALL)\n",
    "header_pattern = re.compile(r\"^△\\s*\")\n",
    "formatted_grammars = []\n",
    "for idx, (grammar, detail) in enumerate(grammars.items()):\n",
    "    try:\n",
    "        content = grammar_start_pattern.sub(\"\", grammar)\n",
    "        hiragana = \"\"\n",
    "        rich_text = transform_content_to_ruby_rich_text(content)\n",
    "        if rich_text != content:\n",
    "            content, hiragana = split_kanji_and_its_kana(content)\n",
    "\n",
    "        example_sentences = detail.get(\"例文\") or []\n",
    "        example_sentences = merge_example_sentences(example_sentences)\n",
    "        formated_exmpale_sentences = []\n",
    "\n",
    "        for sentence_seq, sentence in enumerate(example_sentences):\n",
    "            sentence_content, tag, meaning = example_sentence_pattern.findall(sentence)[0]\n",
    "            sentence_content = header_pattern.sub(\"\", sentence_content)\n",
    "            sentence_content = f\"{get_japanese_sequence_sign(sentence_seq+1)} {sentence_content}\"\n",
    "\n",
    "            sentence_content = transform_content_to_ruby_rich_text(sentence_content)\n",
    "            formated_exmpale_sentences.append({\"content\": sentence_content, \"tag\": tag, \"meaning\": meaning})\n",
    "\n",
    "        # example_sentences = [{\"content\": sentence} for sentence in map(transform_content_to_ruby_rich_text, detail.get(\"例文\") or [])]\n",
    "\n",
    "        formatted_grammars.append({\n",
    "            \"id\": idx + 1,\n",
    "            \"content\": content,\n",
    "            \"hiragana\": hiragana,\n",
    "            \"meaning\": detail.get(\"说明\", \"\").strip(),\n",
    "            \"usage\": transform_content_to_ruby_rich_text(detail.get(\"接续\", \"\").strip()),\n",
    "            \"example\": formated_exmpale_sentences,\n",
    "            \"remark\": transform_content_to_ruby_rich_text(detail.get(\"注意\", \"\").strip()),\n",
    "            \"source\": \"S2N1-N5\",\n",
    "            \"japanese_meaning\": \"\",\n",
    "            \"chinese_meaning\": detail.get(\"说明\", \"\").strip(),\n",
    "            \"level\": detail[\"level\"]\n",
    "        })\n",
    "        if formatted_grammars[-1][\"usage\"] and formatted_grammars[-1][\"usage\"] != transform_content_to_ruby_rich_text(formatted_grammars[-1][\"usage\"]):\n",
    "            empty_usage.append(formatted_grammars[-1])\n",
    "    except Exception:\n",
    "        print(grammar)\n",
    "        print(content)\n",
    "        print(detail)\n",
    "        raise\n",
    "\n",
    "print(formatted_grammars[0])\n",
    "with open(\"grammars_N1-N5.json\", \"w\", encoding=\"utf-8\") as fp:\n",
    "    fp.write(json.dumps(formatted_grammars, ensure_ascii=False, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for detail in empty_usage:\n",
    "    print(detail[\"usage\"])\n",
    "    print(transform_content_to_ruby_rich_text(detail[\"usage\"]))\n",
    "    print(\"=\" * 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 日语平假名unicode编码范围：\\u3040-\\u309F\n",
    "# 日语片假名unicode编码范围：\\u30A0-\\u30FF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = \"私の料理を一口食べるなり 、父は変な顔をして席を立ってしまった。\"\n",
    "get_md5(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from zipfile import ZipFile\n",
    "\n",
    "\n",
    "def extract_epub_images(epub_path, output_folder):\n",
    "    with ZipFile(epub_path, 'r') as epub_file:\n",
    "        for file_name in epub_file.namelist():\n",
    "            if file_name.endswith(('.jpg', '.jpeg', '.png', '.gif')):\n",
    "                output_file_path = os.path.join(output_folder, os.path.basename(file_name))\n",
    "                with open(output_file_path, 'wb') as output_file:\n",
    "                    output_file.write(epub_file.read(file_name))\n",
    "\n",
    "    print(\"Image extraction completed.\")\n",
    "\n",
    "\n",
    "# 示例用法\n",
    "epub_path = japanese_grammar.book_folder  / \"超值白金版.蓝宝书大全集：新日本语能力考试N1-N5文法详解（最新修订版） (许小明) (Z-Library).epub\" # 替换为实际的EPUB文件路径\n",
    "output_folder = japanese_grammar.book_folder / \"temp\"  # 替换为实际的输出文件夹路径\n",
    "\n",
    "extract_epub_images(epub_path, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fpdf import FPDF\n",
    "import os\n",
    "\n",
    "\n",
    "def merge_jpg_to_pdf(folder_path, output_path):\n",
    "    pdf = FPDF()\n",
    "\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith(\".jpg\"):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            pdf.add_page()\n",
    "            pdf.image(file_path, x=0, y=0, w=210)  # 设置图片大小为A4纸尺寸\n",
    "\n",
    "    pdf.output(output_path, \"F\")\n",
    "    print(\"PDF merging completed.\")\n",
    "\n",
    "\n",
    "# 示例用法\n",
    "folder_path = japanese_grammar.book_folder / \"temp\"  # 替换为实际的输出文件夹路径\n",
    "output_path = japanese_grammar.book_folder / \"file.pdf\"  # 替换为实际的输出文件夹路径\n",
    "\n",
    "merge_jpg_to_pdf(folder_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install fpdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import sys\n",
    "\n",
    "kana_meaning_pattern = re.compile(r\"^[△\\u2460-\\u2470\\s]*\")\n",
    "rich_text_pattern = re.compile(r\"<ruby>(.*?)<rt>.*?</rt></ruby>\")\n",
    "RE_WHITESPACES_PATTERN = re.compile(r\"\\s+\")\n",
    "\n",
    "with open(\"grammars_N1-N5.json\", \"r\", encoding=\"utf-8\") as fp:\n",
    "    formatted_grammars = json.loads(fp.read())\n",
    "\n",
    "total_sentence_amount = sum([len(grammar[\"example\"]) for grammar in formatted_grammars])\n",
    "print(f\"Total sentence amount: {total_sentence_amount}\")\n",
    "\n",
    "for grammar in tqdm(formatted_grammars, file=sys.stdout):\n",
    "    for sentence in grammar[\"example\"]:\n",
    "        content = sentence[\"content\"]\n",
    "        content = kana_meaning_pattern.sub(\"\", content)\n",
    "        content = rich_text_pattern.sub(r\"\\1\", content)\n",
    "        content = RE_WHITESPACES_PATTERN.sub(\"\", content)\n",
    "\n",
    "        content_md5 = get_md5(content)\n",
    "        audio_path = f\"japanese_grammar/audio/{content_md5}.mp3\"\n",
    "\n",
    "        text_to_audio.generate_audio([\n",
    "            (None, content),\n",
    "        ], audio_path, log=tqdm.write)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar_content_pattern = re.compile(r\"^\\d? ?[~\\u3040-\\u309F\\u30A0-\\u30FF\\u4E00-\\u9FFF  ()]+\")\n",
    "\n",
    "grammar_content_pattern.findall(\"~ 際 (に)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = \"ここに車を止められるのは、許可をもらっている人（a だけ　b に限り）です。\"\n",
    "content = re.sub(\"\\s+\", \" \", content)\n",
    "print(content)\n",
    "\n",
    "question_pattern = re.compile(r\"[（(](?:[\\da-n]\\s*[~\\u3040-\\u309F\\u30A0-\\u30FF\\u4E00-\\u9FFF]+\\s*)+[)）]\")\n",
    "print(question_pattern.findall(content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(r\"(?:[\\da-n]\\s*[~\\u3040-\\u309F\\u30A0-\\u30FF\\u4E00-\\u9FFF]+\\s*)+\", \"a だけ b に限り\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_pattern = re.compile(r\"[(（](?:[\\da-n]\\s*[~\\u3040-\\u309F\\u30A0-\\u30FF\\u4E00-\\u9FFF]+\\s*)+[）)]*\\?\")\n",
    "print(question_pattern.findall(\"こちらの会議室をご利用になる際は、 受付で必要事項をご記入ください。\"))\n",
    "print(question_pattern.findall(\"ここに車を止められるのは、許可をもらっている人（a だけ　b に限り）です。\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "grammar_content_pattern = re.compile(r\"^\\d [~\\u3040-\\u309F\\u30A0-\\u30FF\\u4E00-\\u9FFF  ()]+\")\n",
    "sentence_content_pattern = re.compile(r'([\\u2460-\\u2470])\\s*')\n",
    "invalid_line_pattern = re.compile(r'^[\\d\\s]+$')\n",
    "\n",
    "example_sentences = []\n",
    "with open(\"CS_Word_20240403_23.41.17.txt\", \"r\", encoding=\"utf-8\") as fp:\n",
    "    matched = False\n",
    "    sentence = None\n",
    "    last_line = \"\"\n",
    "    for line in fp.readlines():\n",
    "        line = line.strip()\n",
    "        if len(line) < 3 or invalid_line_pattern.match(line.strip()):\n",
    "            continue\n",
    "\n",
    "        if grammar_content_pattern.match(line):\n",
    "            print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "law_name_pattern = re.compile(r\"^[一二三四五六七八九十]、.+$\")\n",
    "law_content_pattern = re.compile(r\"^第([零一二三四五六七八九十百]+)条 .+$\")\n",
    "\n",
    "laws_to_answers = defaultdict(lambda : defaultdict(dict))\n",
    "\n",
    "with open(\n",
    "    \"C:/Users/32740/Downloads/福建省幼师考试法律法规填空题答案.txt\", \"r\", encoding=\"utf-8\"\n",
    ") as fp:\n",
    "    lines = fp.readlines()\n",
    "\n",
    "law_name = \"\"\n",
    "law_to_amount = {}\n",
    "count = 0\n",
    "for line in lines:\n",
    "    line = line.strip().strip(\"\\ufeff\")\n",
    "    if not line:\n",
    "        continue\n",
    "\n",
    "    if law_name_pattern.match(line):\n",
    "        if law_name:\n",
    "            law_to_amount[law_name] = count\n",
    "\n",
    "        law_name = line[2:]\n",
    "        count = 0\n",
    "        continue\n",
    "\n",
    "    matched = law_content_pattern.findall(line)\n",
    "    content_idx = chinese_to_number(matched[0])\n",
    "    items = line.split()\n",
    "    answers = items[1].split(\"；\")\n",
    "    laws_to_answers[law_name][content_idx] = answers\n",
    "    count += 1\n",
    "\n",
    "print(law_to_amount)\n",
    "len(laws_to_answers[\"《幼儿园管理条例》\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "invalid_pattern = re.compile(r\"^[\\d]+$\")\n",
    "law_name_pattern = re.compile(r\"^[一二三四五六七八九十]、.+$\")\n",
    "chapter_name_pattern = re.compile(r\"^第[一二三四五六七八九十]+章 .+$\")\n",
    "law_content_pattern = re.compile(r\"^第([零一二三四五六七八九十百]+)条 .+$\")\n",
    "trim_pattern = re.compile(\n",
    "    r\"(?:^[\\s\\ufeff]*(?:2004)?[\\s\\ufeff]*|[\\s\\ufeff]*(?:2004)?[\\s\\ufeff]*$)\"\n",
    ")\n",
    "invalid_keywords = {\n",
    "    \"闽试教师幼儿园教育法律法规重点挖空练习\",\n",
    "}\n",
    "\n",
    "with open(\n",
    "    \"C:/Users/32740/Downloads/福建24届教育法律法规精选重点挖空练习+幼儿园题目.txt\",\n",
    "    \"r\",\n",
    "    encoding=\"utf-8\",\n",
    ") as fp:\n",
    "    lines = fp.readlines()\n",
    "\n",
    "law_name = \"\"\n",
    "chatper_name = \"\"\n",
    "content = []\n",
    "laws = defaultdict(lambda: defaultdict(list))\n",
    "law_to_numbers = defaultdict(set)\n",
    "\n",
    "for idx, line in enumerate(lines):\n",
    "    line = line.strip()\n",
    "    line = trim_pattern.sub(\"\", line)\n",
    "    line = line.strip()\n",
    "    if not line or line in invalid_keywords:\n",
    "        continue\n",
    "\n",
    "    if invalid_pattern.match(line):\n",
    "        print(line)\n",
    "        continue\n",
    "\n",
    "    if law_name_pattern.match(line):\n",
    "        if content:\n",
    "            laws[law_name][chatper_name].append((content_idx, \"\".join(content)))\n",
    "            content = []\n",
    "\n",
    "        law_name = line[2:]\n",
    "        chatper_name = \"\"\n",
    "        # print(line)\n",
    "        continue\n",
    "\n",
    "    if chapter_name_pattern.match(line):\n",
    "        if content:\n",
    "            laws[law_name][chatper_name].append((content_idx, \"\".join(content)))\n",
    "            content = []\n",
    "\n",
    "        chatper_name = line\n",
    "        continue\n",
    "\n",
    "    if matched := law_content_pattern.findall(line):\n",
    "        if content:\n",
    "            laws[law_name][chatper_name].append((content_idx, \"\".join(content)))\n",
    "            \n",
    "        content_idx = chinese_to_number(matched[0])\n",
    "        content = [line]\n",
    "        continue\n",
    "\n",
    "    # print(line)\n",
    "\n",
    "    if not chatper_name:\n",
    "        # print(line)\n",
    "        continue\n",
    "\n",
    "    content.append(line)\n",
    "\n",
    "\n",
    "if content:\n",
    "    laws[law_name][chatper_name].append((content_idx, \"\".join(content)))\n",
    "    content = []\n",
    "\n",
    "law_to_content_number = {\n",
    "    \"《中华人民共和国教育法》（2021年修正）\": 86,\n",
    "    \"《中华人民共和国教师法》（2009年修正）\": 39,\n",
    "    \"《中华人民共和国未成年人保护法》（2020年修订）\": 129,\n",
    "    \"《幼儿园管理条例》\": 29,\n",
    "    \"《教师资格条例》\": 23,\n",
    "    \"《幼儿园工作规程》\": 66,\n",
    "    \"《学生伤害事故处理办法》（2010年修正）\": 40,\n",
    "    \"《中小学幼儿园安全管理办法》\": 64,\n",
    "    \"《中华人民共和国家庭教育促进法》\": 55,\n",
    "}\n",
    "\n",
    "print(len(laws.keys()))\n",
    "\n",
    "# for law_name, law_content in laws.items():\n",
    "#     content_amount = sum(map(len, law_content.values()))\n",
    "#     if content_amount != law_to_content_number[law_name]:\n",
    "#         print(\n",
    "#             f\"{law_name} expected {law_to_content_number[law_name]} but got {content_amount}\"\n",
    "#         )\n",
    "#         # print(\n",
    "#         #     set(range(1, law_to_content_number[law_name] + 1))\n",
    "#         #     - law_to_numbers[law_name]\n",
    "#         # )\n",
    "#     for chatper_name in law_content.keys():\n",
    "#         print(f\"{law_name} - {chatper_name}\")\n",
    "#     print(\"=\" * 32)\n",
    "\n",
    "print(content)\n",
    "print(laws[\"《中华人民共和国教育法》（2021年修正）\"][\"第一章 总则\"])\n",
    "print(laws[\"《中华人民共和国教育法》（2021年修正）\"][\"第二章 教育基本制度\"])\n",
    "print(laws[\"《中华人民共和国教师法》（2009年修正）\"])\n",
    "\n",
    "data = [\n",
    "    {\n",
    "        \"law_name\": law_name,\n",
    "        \"chatper_name\": chatper_name,\n",
    "        \"idx\": item[0],\n",
    "        \"content\": item[1],\n",
    "        \"answer\": json.dumps(laws_to_answers[law_name][item[0]] or [], ensure_ascii=False),\n",
    "    }\n",
    "    for law_name, law_content in laws.items()\n",
    "    for chatper_name, lines in law_content.items()\n",
    "    for item in lines\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(data, dtype=str).fillna(\"\")\n",
    "print(df[df.law_name == \"《中华人民共和国教育法》（2021年修正）\"].chatper_name.unique())\n",
    "df.to_csv(\"C:/Users/32740/Downloads/福建省幼师考试法律法规填空题.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "blank_pattern = re.compile(r\"\\s{2,}\")\n",
    "single_blank_pattern = re.compile(r\"(?:\\s+|_+)\")\n",
    "\n",
    "missing_laws = defaultdict(int)\n",
    "df = pd.read_csv(\n",
    "    \"C:/Users/32740/Downloads/福建省幼师考试法律法规填空题.csv\", dtype=str\n",
    ").fillna(\"\")\n",
    "\n",
    "to_skip = {\n",
    "    \"《中华人民共和国教育法》（2021年修正）\": 13,\n",
    "    \"《中华人民共和国未成年人保护法》（2020年修订）\": 28,\n",
    "    \"《幼儿园工作规程》\": 17,\n",
    "}\n",
    "\n",
    "for index in df.index:\n",
    "    detail = df.loc[index].to_dict()\n",
    "    answer = json.loads(detail[\"answer\"])\n",
    "    if not answer:\n",
    "        continue\n",
    "    content = detail[\"content\"]\n",
    "\n",
    "    first_blank_idx = content.find(\" \")\n",
    "    prefix = content[:first_blank_idx+1]\n",
    "    postfix = content[first_blank_idx+1:]\n",
    "    # matched = blank_pattern.findall(detail[\"content\"])\n",
    "    matched = single_blank_pattern.findall(postfix) \n",
    "    for idx, keyword in enumerate(matched):\n",
    "        postfix = postfix.replace(keyword, \"{{\" + f\"c{idx+1}::{answer[idx]}\" + \"}}\", 1)\n",
    "\n",
    "    content = prefix + postfix\n",
    "    df.loc[index, \"anki_question\"] = content\n",
    "\n",
    "df[df.answer != \"[]\"].to_csv(\"C:/Users/32740/Downloads/anki_福建省幼师考试法律法规填空题.csv\", index=False)\n",
    "missing_laws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laws = list(laws_to_answers)\n",
    "print(laws)\n",
    "print(list(map(lambda law: len(laws_to_answers[law]), laws_to_answers)))\n",
    "print(sum(map(lambda law: len(laws_to_answers[law]), laws_to_answers)))\n",
    "print(df[df.answer != \"[]\"].shape)\n",
    "\n",
    "{\n",
    "    f\"{law}_{answer_idx}\"\n",
    "    for law in laws_to_answers\n",
    "    for answer_idx in laws_to_answers[law]\n",
    "} - {\n",
    "    f\"{df.loc[index, 'law_name']}_{df.loc[index, 'idx']}\"\n",
    "    for index in df.index\n",
    "    if df.loc[index, \"answer\"] != \"[]\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "cookies = {\n",
    "    'BAIDU_SSP_lcr': 'https://www.google.com/',\n",
    "    'ASPSESSIONIDQUBCBADR': 'FIDPCAEAJJDMLFBCJLBCLOCK',\n",
    "    'Hm_lvt_5a5e1257cebb73bd6a288064324f215b': '1718214462',\n",
    "    'Hm_lpvt_5a5e1257cebb73bd6a288064324f215b': '1718214509',\n",
    "}\n",
    "\n",
    "headers = {\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
    "    'Accept-Language': 'en,zh-CN;q=0.9,zh;q=0.8',\n",
    "    'Cache-Control': 'no-cache',\n",
    "    'Connection': 'keep-alive',\n",
    "    # 'Cookie': 'BAIDU_SSP_lcr=https://www.google.com/; ASPSESSIONIDQUBCBADR=FIDPCAEAJJDMLFBCJLBCLOCK; Hm_lvt_5a5e1257cebb73bd6a288064324f215b=1718214462; Hm_lpvt_5a5e1257cebb73bd6a288064324f215b=1718214509',\n",
    "    'Pragma': 'no-cache',\n",
    "    'Referer': 'https://www.google.com/',\n",
    "    'Sec-Fetch-Dest': 'document',\n",
    "    'Sec-Fetch-Mode': 'navigate',\n",
    "    'Sec-Fetch-Site': 'cross-site',\n",
    "    'Sec-Fetch-User': '?1',\n",
    "    'Upgrade-Insecure-Requests': '1',\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36',\n",
    "    'sec-ch-ua': '\"Google Chrome\";v=\"125\", \"Chromium\";v=\"125\", \"Not.A/Brand\";v=\"24\"',\n",
    "    'sec-ch-ua-mobile': '?0',\n",
    "    'sec-ch-ua-platform': '\"Windows\"',\n",
    "}\n",
    "\n",
    "# response = requests.get('https://www.waizi.org.cn/doc/111308.html', \n",
    "response = requests.get('https://www.waizi.org.cn/doc/251916.html', \n",
    "# cookies=cookies, \n",
    "headers=headers,)\n",
    "html_doc = response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "text_container = soup.find(id='text')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = False\n",
    "\n",
    "for child in text_container.find_all(\"p\"):\n",
    "    content = child.text.strip()\n",
    "    if content == \"第一章　总　　则\":\n",
    "        start = True\n",
    "\n",
    "    if start:\n",
    "        print(child.text)\n",
    "\n",
    "    if content.endswith(\"第一百三十二条　本法自2021年6月1日起施行。\"):\n",
    "        start = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "cookies = {\n",
    "    'wdcid': '7caa5ff5ee26505f',\n",
    "    'wdlast': '1718218088',\n",
    "    'wdses': '51460f53be054a9c',\n",
    "}\n",
    "\n",
    "headers = {\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
    "    'Accept-Language': 'en,zh-CN;q=0.9,zh;q=0.8',\n",
    "    'Cache-Control': 'no-cache',\n",
    "    # 'Cookie': 'wdcid=7caa5ff5ee26505f; wdlast=1718218088; wdses=51460f53be054a9c',\n",
    "    'Pragma': 'no-cache',\n",
    "    'Proxy-Connection': 'keep-alive',\n",
    "    'Referer': 'https://www.google.com/',\n",
    "    'Upgrade-Insecure-Requests': '1',\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36',\n",
    "}\n",
    "\n",
    "response = requests.get(\n",
    "    'http://www.moe.gov.cn/srcsite/A02/s5911/moe_621/201602/t20160229_231184.html',\n",
    "    # cookies=cookies,\n",
    "    headers=headers,\n",
    "    verify=False,\n",
    ")\n",
    "html_doc = response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "text_container = soup.find(id='downloadContent')\n",
    "\n",
    "\n",
    "start = False\n",
    "\n",
    "for child in text_container.find_all(\"p\"):\n",
    "    content = child.text.strip()\n",
    "    if content.startswith(\"第一章\"):\n",
    "        start = True\n",
    "\n",
    "    if start:\n",
    "        print(child.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import base64\n",
    "\n",
    "# Configuration https://shibainu.openai.azure.com/\n",
    "GPT4V_KEY = \"f7d7d95881d440dda5f275c5574f04d0\"\n",
    "# IMAGE_PATH = \"YOUR_IMAGE_PATH\"\n",
    "# encoded_image = base64.b64encode(open(IMAGE_PATH, \"rb\").read()).decode(\"ascii\")\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"api-key\": GPT4V_KEY,\n",
    "}\n",
    "\n",
    "# Payload for the request\n",
    "payload = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"You are an AI assistant that helps people find information.\",\n",
    "                }\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_p\": 0.95,\n",
    "    \"max_tokens\": 800,\n",
    "}\n",
    "\n",
    "GPT4V_ENDPOINT = \"https://shibainu.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-02-15-preview\"\n",
    "\n",
    "# Send request\n",
    "try:\n",
    "    response = requests.post(GPT4V_ENDPOINT, headers=headers, json=payload)\n",
    "    response.raise_for_status()  # Will raise an HTTPError if the HTTP request returned an unsuccessful status code\n",
    "except requests.RequestException as e:\n",
    "    raise SystemExit(f\"Failed to make the request. Error: {e}\")\n",
    "\n",
    "# Handle the response as needed (e.g., print or process)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import AzureChatOpenAI\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    openai_api_version=\"f7d7d95881d440dda5f275c5574f04d0\",\n",
    "    azure_deployment=\"gpt-4o\",\n",
    "    temperature=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display, Audio, Markdown\n",
    "import base64\n",
    "\n",
    "IMAGE_PATH = \"C:/Users/32740/Downloads/bk13.png\"\n",
    "\n",
    "# Preview image for context\n",
    "display(Image(IMAGE_PATH))\n",
    "\n",
    "\n",
    "# Open the image file and encode it as a base64 string\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "base64_image = encode_image(IMAGE_PATH)\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful assistant that responds in Markdown.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"Describe the images as an alternative text\"},\n",
    "            {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\"url\": f\"data:image/png;base64,{base64_image}\"},\n",
    "            },\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "ai_message = llm.invoke(messages)\n",
    "print(ai_message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_deployment=\"gpt-4o\",\n",
    "    api_version=\"2024-02-15-preview\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import LLMChain, PromptTemplate\n",
    "\n",
    "sentence_analyzing_prompt_template = \"\"\"\n",
    "    analyze the following Japanese sentence and using Chinese to explain the used grammar and words:\n",
    "    requirement 1: you should use Chinese to reply.\n",
    "    ---\n",
    "    {content}\n",
    "    \"\"\"\n",
    "sentence_analyzing_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=PromptTemplate.from_template(sentence_analyzing_prompt_template),\n",
    "    output_key=\"analysis\",\n",
    ")\n",
    "content = \"お客様あっての仕事ですから、いつもご来店いただくお客様には感謝しております。\"\n",
    "result: str = sentence_analyzing_chain.run(\n",
    "    {\n",
    "        \"content\": content,\n",
    "    }\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import markdown\n",
    "\n",
    "# 示例 Markdown 文本\n",
    "markdown_text = \"\"\"\n",
    "# 这是一个标题\n",
    "\n",
    "这是一个段落，其中包含 **粗体** 和 *斜体* 文本。\n",
    "\n",
    "- 列表项 1\n",
    "- 列表项 2\n",
    "- 列表项 3\n",
    "\n",
    "[这是一个链接](https://www.example.com)\n",
    "\n",
    "```python\n",
    "print(\"这是一个代码块\")\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "html = markdown.markdown(markdown_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader, PdfWriter\n",
    "\n",
    "def crop_pdf(input_pdf_path, crop_ratios, output_pdf_path):\n",
    "    \"\"\"\n",
    "    Crop each page of a PDF file based on the given ratios and save the output to a new file.\n",
    "\n",
    "    :param input_pdf_path: Path to the input PDF file.\n",
    "    :param crop_ratios: A tuple of four ratios (left_ratio, top_ratio, right_ratio, bottom_ratio).\n",
    "                        Each ratio represents the fraction to crop from the corresponding side.\n",
    "    :param output_pdf_path: Path to save the cropped PDF file.\n",
    "    \"\"\"\n",
    "    left_ratio, top_ratio, right_ratio, bottom_ratio = crop_ratios\n",
    "\n",
    "    # Read the input PDF\n",
    "    reader = PdfReader(input_pdf_path)\n",
    "    writer = PdfWriter()\n",
    "\n",
    "    # Iterate through all pages and crop each one\n",
    "    for page in reader.pages:\n",
    "        media_box = page.mediabox\n",
    "\n",
    "        # Get original dimensions\n",
    "        orig_left = float(media_box.left)\n",
    "        orig_bottom = float(media_box.bottom)\n",
    "        orig_right = float(media_box.right)\n",
    "        orig_top = float(media_box.top)\n",
    "\n",
    "        # Calculate new boundaries\n",
    "        new_left = orig_left + (orig_right - orig_left) * left_ratio\n",
    "        new_right = orig_right - (orig_right - orig_left) * right_ratio\n",
    "        new_bottom = orig_bottom + (orig_top - orig_bottom) * bottom_ratio\n",
    "        new_top = orig_top - (orig_top - orig_bottom) * top_ratio\n",
    "\n",
    "        # Set the new dimensions\n",
    "        page.mediabox.lower_left = (new_left, new_bottom)\n",
    "        page.mediabox.upper_right = (new_right, new_top)\n",
    "\n",
    "        # Add the cropped page to the writer\n",
    "        writer.add_page(page)\n",
    "\n",
    "    # Write the cropped pages to a new file\n",
    "    with open(output_pdf_path, 'wb') as out_file:\n",
    "        writer.write(out_file)\n",
    "\n",
    "    print(f\"Cropped PDF saved to {output_pdf_path}\")\n",
    "\n",
    "# Example usage:\n",
    "# crop_pdf(\"input.pdf\", (0.1, 0.1, 0.1, 0.1), \"output.pdf\")\n",
    "\n",
    "\n",
    "\n",
    "# 示例用法\n",
    "input_path = 'G:/BaiduNetdiskDownload/日语真题（1992-2023年12月）/日语N1真题/日语N1真题/【03】N1 2020年-2023年12月/2023.12/2023年12月N1真题.pdf'\n",
    "ratios = (0.05, 0.05, 0.05, 0.075)  # 裁剪比例示例  bottom, left, top, right \n",
    "output_path = 'C:/Users/32740/Downloads/output.pdf'\n",
    "\n",
    "crop_pdf(input_path, ratios, output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def parse_questions(content):\n",
    "    questions = []\n",
    "    current_question = {}\n",
    "    options = []\n",
    "\n",
    "    for line in content.strip().split('\\n'):\n",
    "        line = line.strip()\n",
    "\n",
    "        if line:\n",
    "            if line.isdigit():\n",
    "                if current_question:\n",
    "                    current_question['options'] = options\n",
    "                    questions.append(current_question)\n",
    "                    current_question = {}\n",
    "                    options = []\n",
    "                current_question['number'] = int(line)\n",
    "            elif re.match(r'\\d+\\s+', line):\n",
    "                option_num, option_text = line.split(maxsplit=1)\n",
    "                options.append(option_text)\n",
    "            else:\n",
    "                if 'content' in current_question:\n",
    "                    current_question['content'] += ' ' + line\n",
    "                else:\n",
    "                    current_question['content'] = line\n",
    "\n",
    "    if current_question:\n",
    "        current_question['options'] = options\n",
    "        questions.append(current_question)\n",
    "\n",
    "    return questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "\n",
    "def read_docx(file_path):\n",
    "    doc = Document(file_path)\n",
    "    full_text = []\n",
    "    for paragraph in doc.paragraphs:\n",
    "        full_text.append(paragraph.text)\n",
    "    return '\\n'.join(full_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 示例用法\n",
    "file_path = 'C:/Users/32740/Downloads/CS_Word_20240815_02.15.13.docx'\n",
    "text = read_docx(file_path)\n",
    "# print(text)\n",
    "parse_questions(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_questions(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(r\"問題\\s*\\d\")\n",
    "for line in text.split(\"\\n\"):\n",
    "    if pattern.search(line):\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(r\"\\d+\\s\\D*\")\n",
    "\n",
    "file_path = \"C:/Users/32740/Downloads/CS_Word_20240815_02.15.13.docx\"\n",
    "content = read_docx(file_path)\n",
    "\n",
    "last_line = \"\"\n",
    "all_result = []\n",
    "reading_area = False\n",
    "lines = []\n",
    "for line in content.split(\"\\n\"):\n",
    "    line = line.strip()\n",
    "    if not line:\n",
    "        continue\n",
    "\n",
    "    if line.split()[0] == \"問題1\":\n",
    "        if not reading_area:\n",
    "            reading_area = True\n",
    "\n",
    "    if line.split()[0] == \"問題8\":\n",
    "        break\n",
    "\n",
    "    if not reading_area:\n",
    "        continue\n",
    "\n",
    "    lines.append(line)\n",
    "\n",
    "    # if line[:2] == \"問題\":\n",
    "    #     continue\n",
    "\n",
    "    # if line.isdigit():\n",
    "    #     last_line = line\n",
    "    #     continue\n",
    "\n",
    "    # if last_line.isdigit():\n",
    "    #     line = last_line + \" \" + line\n",
    "\n",
    "    # matched = pattern.findall(line)\n",
    "    # last_line = line\n",
    "\n",
    "    # if not matched:\n",
    "    #     print(line)\n",
    "    # print(matched)\n",
    "\n",
    "    # all_result.extend(matched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\".join(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digit_pattern = re.compile(r\"^\\d\")\n",
    "\n",
    "for index, line in enumerate(lines):\n",
    "    if line[:2] == \"問題\":\n",
    "        lines[index] = \"\"\n",
    "        continue\n",
    "\n",
    "    if not digit_pattern.match(line):\n",
    "        last_line = lines[index - 1]\n",
    "        if not last_line:\n",
    "            lines[index] = \"\"\n",
    "            continue\n",
    "\n",
    "        print(f\"last_line: {last_line}\")\n",
    "        print(f\"line: {line}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022年12月N1真题.pdf: 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "incorrect startxref pointer(1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022年7月N1真题完整版.pdf: 34\n",
      "2023年12月N1真题.pdf: 52\n",
      "2023年7月N1真题 （13页）.pdf: 13\n",
      "N1 2020年12月真题+答案+详解+听力原文.pdf: 31\n",
      "N1 2021年12月真题+答案+详解+听力原文.pdf: 33\n",
      "N1 2021年7月真题+答案+详解+听力原文.pdf: 31\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import PyPDF2\n",
    "\n",
    "\n",
    "def get_pdf_page_count(file_path):\n",
    "    with open(file_path, \"rb\") as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        return len(reader.pages)\n",
    "\n",
    "\n",
    "folder_path = \"G:/Code/aki_api/data\"\n",
    "for file in os.listdir(folder_path):\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    if os.path.isfile(file_path) and file_path.endswith(\".pdf\"):\n",
    "        page_count = get_pdf_page_count(file_path)\n",
    "        print(f\"{file}: {page_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = \"\"\"\n",
    "36 この花火大会は、日本の夏を語る    有名だ。\n",
    "1. うえで\n",
    "2. として\n",
    "3. 欠かせない\n",
    "4.イベント\n",
    "\n",
    "37. 子供にいろいろなことを習わせたいという親の気持ちはよくわかるが、 子供の遊ぶ時間を奪って    ないと思う。\n",
    "1.必要は\n",
    "2. やらせる\n",
    "3.まで\n",
    "4. 無理に\n",
    "\n",
    "38 A「おとといは大雨、 昨日は車のパンク。 旅行に来てからトラブル続きで嫌になるよね。 」 B 「本当だよ。最終日の     。」 \n",
    "1.何も\n",
    "2.今日こそ\n",
    "3.ように\n",
    "4. 起きません\n",
    "\n",
    "39 2025年には介護を必要とする高齢者が著しく増加することから、多くの専門家が 「介護施設職員の給与を引き上げる    と指摘する。\n",
    "1.などして\n",
    "2. 緊急の課題\n",
    "3.人材を確保することが\n",
    "4. 介護に携わる\n",
    "\n",
    "40川北市出身の画家平(たいら) (かず) 明 (あき)の    先週20日に完成した。\n",
    "1.建設を進めて\n",
    "2. 川北市が\n",
    "3. 記念美術館が\n",
    "4. 業績を後世に伝えようと\n",
    "\n",
    "\n",
    "36 そんな簡単なこと、 わざわざあなたに\n",
    "1.もらう\n",
    "2. ない\n",
    "3. 説明して\n",
    "4. までも\n",
    "\n",
    "37 ちょっと考えれば、さっきの話が冗談    単純な彼は簡単に信じてしまった。\n",
    "1. わかるだろう\n",
    "2. だって\n",
    "3.に\n",
    "4. ことくらい\n",
    "\n",
    "38 彼はとても優秀で成績が学年の上位に入っている    真面目で好感が持てる。\n",
    "1.授業に取り組む\n",
    "2. のみならず\n",
    "3. 姿勢そのものも\n",
    "4.ことが多い\n",
    "\n",
    "39. 全国高校バスケットボール大会で、 惜しくも    しばらくの間、ぼう然としていた。\n",
    "1. 優勝を逃した\n",
    "2. 控え室に戻っても\n",
    "3.選手たちは\n",
    "4. あと一歩というところで\n",
    "\n",
    "40. 私が接客するにあたって    自分は何をすべきかということだ。\n",
    "1. 常に考えているのは\n",
    "2. 何であって\n",
    "3. それに応えるために\n",
    "4.お客様が求めていることは\n",
    "\n",
    "36.ステージに     上がった。\n",
    "1. なり\n",
    "2. 登場する\n",
    "3.歓声が\n",
    "4.歌手が\n",
    "\n",
    "37. 彼はこの映画で、 純料で不器用な    までに演じきった。\n",
    "1. その確かな\n",
    "2. 主人公を\n",
    "3. 見事な\n",
    "4. 表現力で\n",
    "\n",
    "\n",
    "38. あの人のことが好きて    いいと思う。\n",
    "1. 忘れられないんだったら\n",
    "2. 何度でも気持ちを伝えたら\n",
    "3. 一度断られたぐらいで\n",
    "4. あきらめたりしないで\n",
    "\n",
    "\n",
    "39.他人のことは     人間というものだ。\n",
    "1. 自分のこととなると\n",
    "2. そう簡単にはいかず\n",
    "3. 感情的になってしまうのが\n",
    "4. 冷静に見られるのに\n",
    "\n",
    "\n",
    "40.介護の現場における    そのうち必ず限界が来る。 介護ロボットの普及が期待される。\n",
    "1. このまま\n",
    "2. 人の手のみに頼っていては\n",
    "3. 人手不足が深刻している\n",
    "4.中で\n",
    "\n",
    "\n",
    "36、(インクビューで) 「私が30年間歌手を続けてこられたのは、ファンの方の支えがあったからです。    歌い続けたいと思っています。」\n",
    "1 限り\n",
    "2 ファンの皆さんが\n",
    "3 応援してくれる\n",
    "4 いる\n",
    "\n",
    "37 S 市が 18歳以上の    運動不足を感じていることがわかった。\n",
    "1 意識調査を行ったところ\n",
    "3 多くの人が\n",
    "2 市民を対象に\n",
    "4 運動に関する\n",
    "\n",
    "\n",
    "38、 子供のころ、 母はしつけに厳しくて、 私はそれが嫌だった。 しかし、 母が    今ならわかる。\n",
    "1 親になった\n",
    "2 私のことを\n",
    "3 思えばこそだったのだと\n",
    "4 厳しかったのは\n",
    "\n",
    "\n",
    "39、 去年    若者の間で流行している。\n",
    "1 レインコートは\n",
    "2 機能性もさることながら\n",
    "3 M社から発売された\n",
    "4 そのかわいらしいデザインが話題となり\n",
    "\n",
    "\n",
    "40、 「未来のものづくりコンテスト」 は、     今年で20回目を迎える。\n",
    "1 子供たちに\n",
    "2 コンテストで\n",
    "3 ものづくりの面白さを感じてもらおうと\n",
    "4 ABC 社が創立50周年を機に始めた\n",
    "\n",
    "36) 秋の初めてのこの時期は、真夏の戻った    気温の差が大きくて体調を崩しやすいので、注意が必要だ。\n",
    "1 日もあり\n",
    "2 日もあれば\n",
    "3 ひんやりとした\n",
    "4 かのような\n",
    "\n",
    "37) 歴史的価値が高いとされる旧白本小学校の校舎を始めて見たが、 とても    驚いた。\n",
    "1 100年前に\n",
    "2 現代的なデザインに\n",
    "3 建てられたものとは\n",
    "4 思えないほどの\n",
    "\n",
    "38)くしゃみや鼻水といった症状が現れる花粉症は、風邪と    ケースもあるそうだ。\n",
    "1  自覚がない\n",
    "2 自分は花粉症だという\n",
    "3 症状が似ている\n",
    "4 こともあって\n",
    "\n",
    "39) 高校卒業後の進路について両親に反対されたが、誰に何と    思う。\n",
    "1 思える道を\n",
    "2 信じて進みたいと\n",
    "3 言われようとも\n",
    "4 自分がこれだと\n",
    "\n",
    "40) 企業の海外進出が成功するかどうかは、    といってもいいだろう。\n",
    "1 優秀な人材を確保できる\n",
    "2 にかかっている\n",
    "3 か否か\n",
    "4 その国の事情をよく知る\n",
    "\n",
    "36 スピーチやプレゼンテーションにおいて、ジェスチャーを使いながら話すのは    印象を悪くすることもある。\n",
    "1 あまりに\n",
    "2 効果的である反面\n",
    "3 かえって\n",
    "4 大きすぎるジェスチャーは\n",
    "\n",
    "37 今や、書類や衣料品だけでなく生鮮食品も、インターネットへの    時代である。\n",
    "1 自宅にいながらにして\n",
    "2 手軽に購入できる\n",
    "3 環境があれば\n",
    "4 アクセスが可能な\n",
    "\n",
    "38 よく似た昆虫の判別は大変難しいという。 中には、 かなり    場合もあるそうだ。\n",
    "1 判別が難しい\n",
    "2 昆虫学者でさえも\n",
    "3 経験を積んだ\n",
    "4 違うほど\n",
    "\n",
    "39 今回の    気持ちの方が大きかった。\n",
    "1 転職にあたり\n",
    "2 うそになるが\n",
    "3 少しも不安がなかったといえば\n",
    "4 新たなことに挑戦できてうれしいという\n",
    "\n",
    "40 北山市は    のどかなところだった。\n",
    "1 人口10万人を超える都市となったが\n",
    "2 北山駅周辺以外にはほとんど何もない\n",
    "3 今でこそ\n",
    "4 30年前までは\n",
    "\n",
    "36 当社が先月発売したパソコンについて、 印刷の    ことが判明しました。\n",
    "1 マニュアルに誤りがある\n",
    "2 設定ができないとの\n",
    "3 問い合わせが複数寄せられ\n",
    "4 調査したところ\n",
    "\n",
    "37 このフライパンは、さすが    とても使いやすい。\n",
    "1 森さんが\n",
    "2 だけあって\n",
    "3 勧める\n",
    "4 料理が上手な\n",
    "\n",
    "38 最後に見た映画が    映画を見ていない。\n",
    "1 ほど\n",
    "2 何だったのかも\n",
    "3 思い出せない\n",
    "4 久しく\n",
    "\n",
    "39 学生から提出された論文の中に面白いものがあった。 私が    これまでにない。\n",
    "1 知る限りでは\n",
    "2 論文は\n",
    "3 分析している\n",
    "4 このアプローチで\n",
    "\n",
    "40 このレシピ本は、ふだん料理をしない人でも    評判になっているそうだ。\n",
    "1 若い人の間で\n",
    "2 調理するだけで\n",
    "3 レシピに沿って\n",
    "4 簡単に本格的な料理が作れると\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
